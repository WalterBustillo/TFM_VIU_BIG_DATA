{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrE-A7XPP0R"
      },
      "source": [
        "#ðŸ“š LibrerÃ­as necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bUWkxi_KnDh"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.7.1 \n",
        "!pip install torchvision==0.8.2 \n",
        "!pip install matplotlib \n",
        "!pip install scikit-image \n",
        "!pip install opencv-python\n",
        "!pip install yacs\n",
        "!pip install joblib \n",
        "!pip install natsort \n",
        "!pip install h5py \n",
        "!pip install tqdm\n",
        "!pip install einops\n",
        "!pip install linformer\n",
        "!pip install timm\n",
        "!pip install ptflops\n",
        "!pip install dataclasses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYxYYYagsXIy"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCq_VuOnsOao"
      },
      "source": [
        "# ðŸ¤– Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67RiT1cnsQZu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Uformer: A General U-Shaped Transformer for Image Restoration\n",
        "## Zhendong Wang, Xiaodong Cun, Jianmin Bao, Jianzhuang Liu\n",
        "## https://arxiv.org/abs/2106.03106\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.checkpoint as checkpoint\n",
        "import collections.abc as container_abcs\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "from torch import einsum\n",
        "\n",
        "# Limpiamos la cachÃ© de CUDA\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "#########################################\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, strides=1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.strides = strides\n",
        "        self.in_channel=in_channel\n",
        "        self.out_channel=out_channel\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=strides, padding=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=strides, padding=1),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "        )\n",
        "        self.conv11 = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=strides, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.block(x)\n",
        "        out2 = self.conv11(x)\n",
        "        out = out1 + out2\n",
        "        return out\n",
        "\n",
        "    def flops(self, H, W): \n",
        "        flops = H*W*self.in_channel*self.out_channel*(3*3+1)+H*W*self.out_channel*self.out_channel*3*3\n",
        "        return flops\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, block=ConvBlock,dim=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.ConvBlock1 = ConvBlock(3, dim, strides=1)\n",
        "        self.pool1 = nn.Conv2d(dim,dim,kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.ConvBlock2 = block(dim, dim*2, strides=1)\n",
        "        self.pool2 = nn.Conv2d(dim*2,dim*2,kernel_size=4, stride=2, padding=1)\n",
        "       \n",
        "        self.ConvBlock3 = block(dim*2, dim*4, strides=1)\n",
        "        self.pool3 = nn.Conv2d(dim*4,dim*4,kernel_size=4, stride=2, padding=1)\n",
        "       \n",
        "        self.ConvBlock4 = block(dim*4, dim*8, strides=1)\n",
        "        self.pool4 = nn.Conv2d(dim*8, dim*8,kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.ConvBlock5 = block(dim*8, dim*16, strides=1)\n",
        "\n",
        "        self.upv6 = nn.ConvTranspose2d(dim*16, dim*8, 2, stride=2)\n",
        "        self.ConvBlock6 = block(dim*16, dim*8, strides=1)\n",
        "\n",
        "        self.upv7 = nn.ConvTranspose2d(dim*8, dim*4, 2, stride=2)\n",
        "        self.ConvBlock7 = block(dim*8, dim*4, strides=1)\n",
        "\n",
        "        self.upv8 = nn.ConvTranspose2d(dim*4, dim*2, 2, stride=2)\n",
        "        self.ConvBlock8 = block(dim*4, dim*2, strides=1)\n",
        "\n",
        "        self.upv9 = nn.ConvTranspose2d(dim*2, dim, 2, stride=2)\n",
        "        self.ConvBlock9 = block(dim*2, dim, strides=1)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(dim, 3, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.ConvBlock1(x)\n",
        "        pool1 = self.pool1(conv1)\n",
        "\n",
        "        conv2 = self.ConvBlock2(pool1)\n",
        "        pool2 = self.pool2(conv2)\n",
        "\n",
        "        conv3 = self.ConvBlock3(pool2)\n",
        "        pool3 = self.pool3(conv3)\n",
        "\n",
        "        conv4 = self.ConvBlock4(pool3)\n",
        "        pool4 = self.pool4(conv4)\n",
        "\n",
        "        conv5 = self.ConvBlock5(pool4)\n",
        "\n",
        "        up6 = self.upv6(conv5)\n",
        "        up6 = torch.cat([up6, conv4], 1)\n",
        "        conv6 = self.ConvBlock6(up6)\n",
        "\n",
        "        up7 = self.upv7(conv6)\n",
        "        up7 = torch.cat([up7, conv3], 1)\n",
        "        conv7 = self.ConvBlock7(up7)\n",
        "\n",
        "        up8 = self.upv8(conv7)\n",
        "        up8 = torch.cat([up8, conv2], 1)\n",
        "        conv8 = self.ConvBlock8(up8)\n",
        "\n",
        "        up9 = self.upv9(conv8)\n",
        "        up9 = torch.cat([up9, conv1], 1)\n",
        "        conv9 = self.ConvBlock9(up9)\n",
        "\n",
        "        conv10 = self.conv10(conv9)\n",
        "        out = x + conv10\n",
        "\n",
        "        return out\n",
        "\n",
        "    def flops(self, H, W): \n",
        "        flops = 0\n",
        "        flops += self.ConvBlock1.flops(H, W)\n",
        "        flops += H/2*W/2*self.dim*self.dim*4*4\n",
        "        flops += self.ConvBlock2.flops(H/2, W/2)\n",
        "        flops += H/4*W/4*self.dim*2*self.dim*2*4*4\n",
        "        flops += self.ConvBlock3.flops(H/4, W/4)\n",
        "        flops += H/8*W/8*self.dim*4*self.dim*4*4*4\n",
        "        flops += self.ConvBlock4.flops(H/8, W/8)\n",
        "        flops += H/16*W/16*self.dim*8*self.dim*8*4*4\n",
        "\n",
        "        flops += self.ConvBlock5.flops(H/16, W/16)\n",
        "\n",
        "        flops += H/8*W/8*self.dim*16*self.dim*8*2*2\n",
        "        flops += self.ConvBlock6.flops(H/8, W/8)\n",
        "        flops += H/4*W/4*self.dim*8*self.dim*4*2*2\n",
        "        flops += self.ConvBlock7.flops(H/4, W/4)\n",
        "        flops += H/2*W/2*self.dim*4*self.dim*2*2*2\n",
        "        flops += self.ConvBlock8.flops(H/2, W/2)\n",
        "        flops += H*W*self.dim*2*self.dim*2*2\n",
        "        flops += self.ConvBlock9.flops(H, W)\n",
        "\n",
        "        flops += H*W*self.dim*3*3*3\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "class PosCNN(nn.Module):\n",
        "    def __init__(self, in_chans, embed_dim=768, s=1):\n",
        "        super(PosCNN, self).__init__()\n",
        "        self.proj = nn.Sequential(nn.Conv2d(in_chans, embed_dim, 3, s, 1, bias=True, groups=embed_dim))\n",
        "        self.s = s\n",
        "\n",
        "    def forward(self, x, H=None, W=None):\n",
        "        B, N, C = x.shape\n",
        "        H = H or int(math.sqrt(N))\n",
        "        W = W or int(math.sqrt(N))\n",
        "        feat_token = x\n",
        "        cnn_feat = feat_token.transpose(1, 2).view(B, C, H, W)\n",
        "        if self.s == 1:\n",
        "            x = self.proj(cnn_feat) + cnn_feat\n",
        "        else:\n",
        "            x = self.proj(cnn_feat)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def no_weight_decay(self):\n",
        "        return ['proj.%d.weight' % i for i in range(4)]\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: [B, N, C]\n",
        "        x = torch.transpose(x, 1, 2)  # [B, C, N]\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        x = x * y.expand_as(x)\n",
        "        x = torch.transpose(x, 1, 2)  # [B, N, C]\n",
        "        return x\n",
        "\n",
        "class SepConv2d(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 padding=0,\n",
        "                 dilation=1,act_layer=nn.ReLU):\n",
        "        super(SepConv2d, self).__init__()\n",
        "        self.depthwise = torch.nn.Conv2d(in_channels,\n",
        "                                         in_channels,\n",
        "                                         kernel_size=kernel_size,\n",
        "                                         stride=stride,\n",
        "                                         padding=padding,\n",
        "                                         dilation=dilation,\n",
        "                                         groups=in_channels)\n",
        "        self.pointwise = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.act_layer = act_layer() if act_layer is not None else nn.Identity()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.act_layer(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W): \n",
        "        flops = 0\n",
        "        flops += H*W*self.in_channels*self.kernel_size**2/self.stride**2\n",
        "        flops += H*W*self.in_channels*self.out_channels\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "######## Embedding for q,k,v ########\n",
        "class ConvProjection(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, kernel_size=3, q_stride=1, k_stride=1, v_stride=1, dropout = 0.,\n",
        "                 last_stage=False,bias=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        pad = (kernel_size - q_stride)//2\n",
        "        self.to_q = SepConv2d(dim, inner_dim, kernel_size, q_stride, pad, bias)\n",
        "        self.to_k = SepConv2d(dim, inner_dim, kernel_size, k_stride, pad, bias)\n",
        "        self.to_v = SepConv2d(dim, inner_dim, kernel_size, v_stride, pad, bias)\n",
        "\n",
        "    def forward(self, x, attn_kv=None):\n",
        "        b, n, c, h = *x.shape, self.heads\n",
        "        l = int(math.sqrt(n))\n",
        "        w = int(math.sqrt(n))\n",
        "\n",
        "        attn_kv = x if attn_kv is None else attn_kv\n",
        "        x = rearrange(x, 'b (l w) c -> b c l w', l=l, w=w)\n",
        "        attn_kv = rearrange(attn_kv, 'b (l w) c -> b c l w', l=l, w=w)\n",
        "        # print(attn_kv)\n",
        "        q = self.to_q(x)\n",
        "        q = rearrange(q, 'b (h d) l w -> b h (l w) d', h=h)\n",
        "        \n",
        "        k = self.to_k(attn_kv)\n",
        "        v = self.to_v(attn_kv)\n",
        "        k = rearrange(k, 'b (h d) l w -> b h (l w) d', h=h)\n",
        "        v = rearrange(v, 'b (h d) l w -> b h (l w) d', h=h)\n",
        "        return q,k,v    \n",
        "    \n",
        "    def flops(self, H, W): \n",
        "        flops = 0\n",
        "        flops += self.to_q.flops(H, W)\n",
        "        flops += self.to_k.flops(H, W)\n",
        "        flops += self.to_v.flops(H, W)\n",
        "        return flops\n",
        "\n",
        "class LinearProjection(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., bias=True):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias = bias)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = bias)\n",
        "        self.dim = dim\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "    def forward(self, x, attn_kv=None):\n",
        "        B_, N, C = x.shape\n",
        "        attn_kv = x if attn_kv is None else attn_kv\n",
        "        q = self.to_q(x).reshape(B_, N, 1, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        kv = self.to_kv(attn_kv).reshape(B_, N, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        q = q[0]\n",
        "        k, v = kv[0], kv[1] \n",
        "        return q,k,v\n",
        "\n",
        "    def flops(self, H, W): \n",
        "        flops = H*W*self.dim*self.inner_dim*3\n",
        "        return flops \n",
        "\n",
        "class LinearProjection_Concat_kv(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., bias=True):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head *  heads\n",
        "        self.heads = heads\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = bias)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = bias)\n",
        "        self.dim = dim\n",
        "        self.inner_dim = inner_dim\n",
        "\n",
        "    def forward(self, x, attn_kv=None):\n",
        "        B_, N, C = x.shape\n",
        "        attn_kv = x if attn_kv is None else attn_kv\n",
        "        qkv_dec = self.to_qkv(x).reshape(B_, N, 3, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        kv_enc = self.to_kv(attn_kv).reshape(B_, N, 2, self.heads, C // self.heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k_d, v_d = qkv_dec[0], qkv_dec[1], qkv_dec[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "        k_e, v_e = kv_enc[0], kv_enc[1] \n",
        "        k = torch.cat((k_d,k_e),dim=2)\n",
        "        v = torch.cat((v_d,v_e),dim=2)\n",
        "        return q,k,v\n",
        "\n",
        "    def flops(self, H, W): \n",
        "        flops = H*W*self.dim*self.inner_dim*5\n",
        "        return flops \n",
        "\n",
        "#########################################\n",
        "########### window-based self-attention #############\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, win_size,num_heads, token_projection='linear', qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,se_layer=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.win_size = win_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        # define a parameter table of relative position bias\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * win_size[0] - 1) * (2 * win_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        coords_h = torch.arange(self.win_size[0]) # [0,...,Wh-1]\n",
        "        coords_w = torch.arange(self.win_size[1]) # [0,...,Ww-1]\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
        "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "        relative_coords[:, :, 0] += self.win_size[0] - 1  # shift to start from 0\n",
        "        relative_coords[:, :, 1] += self.win_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.win_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "\n",
        "        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        if token_projection =='conv':\n",
        "            self.qkv = ConvProjection(dim,num_heads,dim//num_heads,bias=qkv_bias)\n",
        "        elif token_projection =='linear_concat':\n",
        "            self.qkv = LinearProjection_Concat_kv(dim,num_heads,dim//num_heads,bias=qkv_bias)\n",
        "        else:\n",
        "            self.qkv = LinearProjection(dim,num_heads,dim//num_heads,bias=qkv_bias)\n",
        "        \n",
        "        self.token_projection = token_projection\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.se_layer = SELayer(dim) if se_layer else nn.Identity()\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, attn_kv=None, mask=None):\n",
        "        B_, N, C = x.shape\n",
        "        q, k, v = self.qkv(x,attn_kv)\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.win_size[0] * self.win_size[1], self.win_size[0] * self.win_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        ratio = attn.size(-1)//relative_position_bias.size(-1)\n",
        "        relative_position_bias = repeat(relative_position_bias, 'nH l c -> nH l (c d)', d = ratio)\n",
        "        \n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            mask = repeat(mask, 'nW m n -> nW m (n d)',d = ratio)\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N*ratio) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N*ratio)\n",
        "            attn = self.softmax(attn)\n",
        "        else:\n",
        "            attn = self.softmax(attn)\n",
        "\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.se_layer(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f'dim={self.dim}, win_size={self.win_size}, num_heads={self.num_heads}'\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        # calculate flops for 1 window with token length of N\n",
        "        # print(N, self.dim)\n",
        "        flops = 0\n",
        "        N = self.win_size[0]*self.win_size[1]\n",
        "        nW = H*W/N\n",
        "        # qkv = self.qkv(x)\n",
        "        # flops += N * self.dim * 3 * self.dim\n",
        "        flops += self.qkv.flops(H, W)\n",
        "        # attn = (q @ k.transpose(-2, -1))\n",
        "        if self.token_projection !='linear_concat':\n",
        "            flops += nW * self.num_heads * N * (self.dim // self.num_heads) * N\n",
        "            #  x = (attn @ v)\n",
        "            flops += nW * self.num_heads * N * N * (self.dim // self.num_heads)\n",
        "        else:\n",
        "            flops += nW * self.num_heads * N * (self.dim // self.num_heads) * N*2\n",
        "            #  x = (attn @ v)\n",
        "            flops += nW * self.num_heads * N * N*2 * (self.dim // self.num_heads)\n",
        "        # x = self.proj(x)\n",
        "        flops += nW * N * self.dim * self.dim\n",
        "        print(\"W-MSA:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "########### feed-forward network #############\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "        self.in_features = in_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # fc1\n",
        "        flops += H*W*self.in_features*self.hidden_features \n",
        "        # fc2\n",
        "        flops += H*W*self.hidden_features*self.out_features\n",
        "        print(\"MLP:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "class LeFF(nn.Module):\n",
        "    def __init__(self, dim=32, hidden_dim=128, act_layer=nn.GELU,drop = 0.):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Sequential(nn.Linear(dim, hidden_dim),\n",
        "                                act_layer())\n",
        "        self.dwconv = nn.Sequential(nn.Conv2d(hidden_dim,hidden_dim,groups=hidden_dim,kernel_size=3,stride=1,padding=1),\n",
        "                        act_layer())\n",
        "        self.linear2 = nn.Sequential(nn.Linear(hidden_dim, dim))\n",
        "        self.dim = dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # bs x hw x c\n",
        "        bs, hw, c = x.size()\n",
        "        hh = int(math.sqrt(hw))\n",
        "\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        # spatial restore\n",
        "        x = rearrange(x, ' b (h w) (c) -> b c h w ', h = hh, w = hh)\n",
        "        # bs,hidden_dim,32x32\n",
        "\n",
        "        x = self.dwconv(x)\n",
        "\n",
        "        # flaten\n",
        "        x = rearrange(x, ' b c h w -> b (h w) c', h = hh, w = hh)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # fc1\n",
        "        flops += H*W*self.dim*self.hidden_dim \n",
        "        # dwconv\n",
        "        flops += H*W*self.hidden_dim*3*3\n",
        "        # fc2\n",
        "        flops += H*W*self.hidden_dim*self.dim\n",
        "        print(\"LeFF:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "########### window operation#############\n",
        "def window_partition(x, win_size, dilation_rate=1):\n",
        "    B, H, W, C = x.shape\n",
        "    if dilation_rate !=1:\n",
        "        x = x.permute(0,3,1,2) # B, C, H, W\n",
        "        assert type(dilation_rate) is int, 'dilation_rate should be a int'\n",
        "        x = F.unfold(x, kernel_size=win_size,dilation=dilation_rate,padding=4*(dilation_rate-1),stride=win_size) # B, C*Wh*Ww, H/Wh*W/Ww\n",
        "        windows = x.permute(0,2,1).contiguous().view(-1, C, win_size, win_size) # B' ,C ,Wh ,Ww\n",
        "        windows = windows.permute(0,2,3,1).contiguous() # B' ,Wh ,Ww ,C\n",
        "    else:\n",
        "        x = x.view(B, H // win_size, win_size, W // win_size, win_size, C)\n",
        "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, win_size, win_size, C) # B' ,Wh ,Ww ,C\n",
        "    return windows\n",
        "\n",
        "def window_reverse(windows, win_size, H, W, dilation_rate=1):\n",
        "    # B' ,Wh ,Ww ,C\n",
        "    B = int(windows.shape[0] / (H * W / win_size / win_size))\n",
        "    x = windows.view(B, H // win_size, W // win_size, win_size, win_size, -1)\n",
        "    if dilation_rate !=1:\n",
        "        x = windows.permute(0,5,3,4,1,2).contiguous() # B, C*Wh*Ww, H/Wh*W/Ww\n",
        "        x = F.fold(x, (H, W), kernel_size=win_size, dilation=dilation_rate, padding=4*(dilation_rate-1),stride=win_size)\n",
        "    else:\n",
        "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "#########################################\n",
        "# Downsample Block\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        # import pdb;pdb.set_trace()\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "        out = self.conv(x).flatten(2).transpose(1,2).contiguous()  # B H*W C\n",
        "        return out\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # conv\n",
        "        flops += H/2*W/2*self.in_channel*self.out_channel*4*4\n",
        "        print(\"Downsample:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "# Upsample Block\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(Upsample, self).__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channel, out_channel, kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).contiguous().view(B, C, H, W)\n",
        "        out = self.deconv(x).flatten(2).transpose(1,2).contiguous() # B H*W C\n",
        "        return out\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # conv\n",
        "        flops += H*2*W*2*self.in_channel*self.out_channel*2*2 \n",
        "        print(\"Upsample:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "# Input Projection\n",
        "class InputProj(nn.Module):\n",
        "    def __init__(self, in_channel=3, out_channel=64, kernel_size=3, stride=1, norm_layer=None,act_layer=nn.LeakyReLU):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=kernel_size//2),\n",
        "            act_layer(inplace=True)\n",
        "        )\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(out_channel)\n",
        "        else:\n",
        "            self.norm = None\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2).contiguous()  # B H*W C\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # conv\n",
        "        flops += H*W*self.in_channel*self.out_channel*3*3\n",
        "\n",
        "        if self.norm is not None:\n",
        "            flops += H*W*self.out_channel \n",
        "        print(\"Input_proj:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "# Output Projection\n",
        "class OutputProj(nn.Module):\n",
        "    def __init__(self, in_channel=64, out_channel=3, kernel_size=3, stride=1, norm_layer=None,act_layer=None):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=kernel_size//2),\n",
        "        )\n",
        "        if act_layer is not None:\n",
        "            self.proj.add_module(act_layer(inplace=True))\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(out_channel)\n",
        "        else:\n",
        "            self.norm = None\n",
        "        self.in_channel = in_channel\n",
        "        self.out_channel = out_channel\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "        x = x.transpose(1, 2).view(B, C, H, W)\n",
        "        x = self.proj(x)\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, H, W):\n",
        "        flops = 0\n",
        "        # conv\n",
        "        flops += H*W*self.in_channel*self.out_channel*3*3\n",
        "\n",
        "        if self.norm is not None:\n",
        "            flops += H*W*self.out_channel \n",
        "        print(\"Output_proj:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "########### LeWinTransformer #############\n",
        "class LeWinTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, win_size=8, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,token_projection='linear',token_mlp='leff',se_layer=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.win_size = win_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_mlp = token_mlp\n",
        "        if min(self.input_resolution) <= self.win_size:\n",
        "            self.shift_size = 0\n",
        "            self.win_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.win_size, \"shift_size must in 0-win_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, win_size=to_2tuple(self.win_size), num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,\n",
        "            token_projection=token_projection,se_layer=se_layer)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,act_layer=act_layer, drop=drop) if token_mlp=='ffn' else LeFF(dim,mlp_hidden_dim,act_layer=act_layer, drop=drop)\n",
        "\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"win_size={self.win_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "\n",
        "        ## input mask\n",
        "        if mask != None:\n",
        "            input_mask = F.interpolate(mask, size=(H,W)).permute(0,2,3,1)\n",
        "            input_mask_windows = window_partition(input_mask, self.win_size) # nW, win_size, win_size, 1\n",
        "            attn_mask = input_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            attn_mask = attn_mask.unsqueeze(2)*attn_mask.unsqueeze(1) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        ## shift mask\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            shift_mask = torch.zeros((1, H, W, 1)).type_as(x)\n",
        "            h_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    shift_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "            shift_mask_windows = window_partition(shift_mask, self.win_size)  # nW, win_size, win_size, 1\n",
        "            shift_mask_windows = shift_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            shift_attn_mask = shift_mask_windows.unsqueeze(1) - shift_mask_windows.unsqueeze(2) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask or shift_attn_mask\n",
        "            attn_mask = attn_mask.masked_fill(shift_attn_mask != 0, float(-100.0))\n",
        "            \n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.win_size)  # nW*B, win_size, win_size, C  N*C->C\n",
        "        x_windows = x_windows.view(-1, self.win_size * self.win_size, C)  # nW*B, win_size*win_size, C\n",
        "\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, win_size*win_size, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.win_size, self.win_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.win_size, H, W)  # B H' W' C\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "\n",
        "        # FFN\n",
        "        x = shortcut + self.drop_path(x)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        del attn_mask\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        flops += self.attn.flops(H, W)\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        # mlp\n",
        "        flops += self.mlp.flops(H,W)\n",
        "        print(\"LeWin:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "########### LeWinTransformer_Cross #############\n",
        "class LeWinTransformer_Cross(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, win_size=8, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,token_projection='linear',token_mlp='ffn'):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.win_size = win_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.win_size:\n",
        "            self.shift_size = 0\n",
        "            self.win_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.win_size, \"shift_size must in 0-win_size\"\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = WindowAttention(\n",
        "            dim, win_size=to_2tuple(self.win_size), num_heads=num_heads,qkv_bias=qkv_bias, \n",
        "            qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,token_projection=token_projection)\n",
        "\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        self.norm_kv = norm_layer(dim)\n",
        "        self.cross_attn = WindowAttention(\n",
        "            dim, win_size=to_2tuple(self.win_size), num_heads=num_heads,qkv_bias=qkv_bias, \n",
        "            qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,token_projection=token_projection)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm3 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,act_layer=act_layer, drop=drop) if token_mlp=='ffn' else LeFF(dim,mlp_hidden_dim,act_layer=act_layer, drop=drop)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"win_size={self.win_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def forward(self, x, attn_kv=None, mask=None):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "\n",
        "        ## input mask\n",
        "        if mask != None:\n",
        "            input_mask = F.interpolate(mask, size=(H,W)).permute(0,2,3,1)\n",
        "            input_mask_windows = window_partition(input_mask, self.win_size) # nW, win_size, win_size, 1\n",
        "            attn_mask = input_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            attn_mask = attn_mask.unsqueeze(2)*attn_mask.unsqueeze(1) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        ## shift mask\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            shift_mask = torch.zeros((1, H, W, 1)).type_as(x)\n",
        "            h_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    shift_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "            shift_mask_windows = window_partition(shift_mask, self.win_size)  # nW, win_size, win_size, 1\n",
        "            shift_mask_windows = shift_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            shift_attn_mask = shift_mask_windows.unsqueeze(1) - shift_mask_windows.unsqueeze(2) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask or shift_attn_mask\n",
        "            attn_mask = attn_mask.masked_fill(shift_attn_mask != 0, float(-100.0))\n",
        "        \n",
        "        attn_kv = attn_kv.view(B, H, W, C)\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_kv = torch.roll(attn_kv, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_kv = attn_kv\n",
        "        # partition windows\n",
        "        attn_kv_windows = window_partition(shifted_kv, self.win_size)  # nW*B, win_size, win_size, C\n",
        "        attn_kv_windows = attn_kv_windows.view(-1, self.win_size * self.win_size, C)  # nW*B, win_size*win_size, C\n",
        "         \n",
        "        x = x.view(B, H, W, C)\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.win_size)  # nW*B, win_size, win_size, C\n",
        "        x_windows = x_windows.view(-1, self.win_size * self.win_size, C)  # nW*B, win_size*win_size, C\n",
        "\n",
        "        ### multi-head self-attention \n",
        "        shortcut1 = x_windows \n",
        "        # prenorm\n",
        "        x_windows = self.norm1(x_windows)\n",
        "        # W-MSA/SW-MSA\n",
        "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, win_size*win_size, C\n",
        "        x_windows = shortcut1 + self.drop_path(attn_windows)\n",
        "        \n",
        "        ### multi-head cross-attention\n",
        "        shortcut2 = x_windows\n",
        "        # prenorm\n",
        "        x_windows = self.norm2(x_windows)\n",
        "        attn_kv_windows = self.norm_kv(attn_kv_windows)\n",
        "        # W-MCA/SW-MCA\n",
        "        attn_windows = self.cross_attn(x_windows, attn_kv=attn_kv_windows,mask=attn_mask)  # nW*B, win_size*win_size, C\n",
        "        attn_windows = shortcut2 + self.drop_path(attn_windows)\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.win_size, self.win_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.win_size, H, W)  # B H' W' C\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "        # FFN\n",
        "        x = x + self.drop_path(self.mlp(self.norm3(x)))\n",
        "        del attn_mask\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        flops += self.attn.flops(H, W)\n",
        "        flops += self.cross_attn.flops(H, W)\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        # mlp\n",
        "        flops += self.mlp.flops(H,W)\n",
        "        print(\"LeWin:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "########### LeWinTransformer_CatCross #############\n",
        "class LeWinTransformer_CatCross(nn.Module):\n",
        "    def __init__(self, dim, input_resolution, num_heads, win_size=8, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,token_projection='linear',token_mlp='ffn'):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.win_size = win_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.win_size:\n",
        "            self.shift_size = 0\n",
        "            self.win_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.win_size, \"shift_size must in 0-win_size\"\n",
        "      \n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.norm_kv = norm_layer(dim)\n",
        "        self.cross_attn = WindowAttention(\n",
        "            dim, win_size=to_2tuple(self.win_size), num_heads=num_heads,qkv_bias=qkv_bias, \n",
        "            qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop,token_projection='linear_concat')\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,act_layer=act_layer, drop=drop) if token_mlp=='ffn' else LeFF(dim,mlp_hidden_dim,act_layer=act_layer, drop=drop)\n",
        "\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
        "               f\"win_size={self.win_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
        "\n",
        "    def forward(self, x, attn_kv=None, mask=None):\n",
        "        B, L, C = x.shape\n",
        "        H = int(math.sqrt(L))\n",
        "        W = int(math.sqrt(L))\n",
        "\n",
        "        ## input mask\n",
        "        if mask != None:\n",
        "            input_mask = F.interpolate(mask, size=(H,W)).permute(0,2,3,1)\n",
        "            input_mask_windows = window_partition(input_mask, self.win_size) # nW, win_size, win_size, 1\n",
        "            attn_mask = input_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            attn_mask = attn_mask.unsqueeze(2)*attn_mask.unsqueeze(1) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        ## shift mask\n",
        "        if self.shift_size > 0:\n",
        "            # calculate attention mask for SW-MSA\n",
        "            shift_mask = torch.zeros((1, H, W, 1)).type_as(x)\n",
        "            h_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            w_slices = (slice(0, -self.win_size),\n",
        "                        slice(-self.win_size, -self.shift_size),\n",
        "                        slice(-self.shift_size, None))\n",
        "            cnt = 0\n",
        "            for h in h_slices:\n",
        "                for w in w_slices:\n",
        "                    shift_mask[:, h, w, :] = cnt\n",
        "                    cnt += 1\n",
        "            shift_mask_windows = window_partition(shift_mask, self.win_size)  # nW, win_size, win_size, 1\n",
        "            shift_mask_windows = shift_mask_windows.view(-1, self.win_size * self.win_size) # nW, win_size*win_size\n",
        "            shift_attn_mask = shift_mask_windows.unsqueeze(1) - shift_mask_windows.unsqueeze(2) # nW, win_size*win_size, win_size*win_size\n",
        "            attn_mask = attn_mask or shift_attn_mask\n",
        "            attn_mask = attn_mask.masked_fill(shift_attn_mask != 0, float(-100.0))\n",
        "        \n",
        "        attn_kv = attn_kv.view(B, H, W, C)\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_kv = torch.roll(attn_kv, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_kv = attn_kv\n",
        "        # partition windows\n",
        "        attn_kv_windows = window_partition(shifted_kv, self.win_size)  # nW*B, win_size, win_size, C\n",
        "        attn_kv_windows = attn_kv_windows.view(-1, self.win_size * self.win_size, C)  # nW*B, win_size*win_size, C\n",
        "         \n",
        "        x = x.view(B, H, W, C)\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.win_size)  # nW*B, win_size, win_size, C\n",
        "        x_windows = x_windows.view(-1, self.win_size * self.win_size, C)  # nW*B, win_size*win_size, C\n",
        "\n",
        "        ### multi-head cross-attention\n",
        "        shortcut1 = x_windows\n",
        "        # prenorm\n",
        "        x_windows = self.norm1(x_windows)\n",
        "        attn_kv_windows = self.norm_kv(attn_kv_windows)\n",
        "        # W-MCA/SW-MCA\n",
        "        attn_windows = self.cross_attn(x_windows, attn_kv=attn_kv_windows,mask=attn_mask)  # nW*B, win_size*win_size, C\n",
        "        attn_windows = shortcut1 + self.drop_path(attn_windows)\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.win_size, self.win_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.win_size, H, W)  # B H' W' C\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "        x = x.view(B, H * W, C)\n",
        "        # FFN\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        del attn_mask\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        H, W = self.input_resolution\n",
        "        # norm1\n",
        "        flops += self.dim * H * W\n",
        "        # W-MSA/SW-MSA\n",
        "        flops += self.cross_attn.flops(H, W)\n",
        "        # norm2\n",
        "        flops += self.dim * H * W\n",
        "        # mlp\n",
        "        flops += self.mlp.flops(H,W)\n",
        "        print(\"LeWin:{%.2f}\"%(flops/1e9))\n",
        "        return flops\n",
        "\n",
        "#########################################\n",
        "########### Basic layer of Uformer ################\n",
        "class BasicUformerLayer(nn.Module):\n",
        "    def __init__(self, dim, output_dim, input_resolution, depth, num_heads, win_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, use_checkpoint=False,\n",
        "                 token_projection='linear',token_mlp='ffn',se_layer=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        # build blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            LeWinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, win_size=win_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else win_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer,token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"    \n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x,mask)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        return flops\n",
        "\n",
        "########### Basic decoderlayer of Uformer_Cross ################\n",
        "class CrossUformerLayer(nn.Module):\n",
        "    def __init__(self, dim, output_dim, input_resolution, depth, num_heads, win_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, use_checkpoint=False,\n",
        "                 token_projection='linear',token_mlp='ffn'):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        # build blocks\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            LeWinTransformer_Cross(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, win_size=win_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else win_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer,token_projection=token_projection,token_mlp=token_mlp)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
        "        \n",
        "    def forward(self, x, attn_kv=None, mask=None):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x,attn_kv,mask)\n",
        "        return x \n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        return flops\n",
        "\n",
        "########### Basic decoderlayer of Uformer_CatCross ################\n",
        "class CatCrossUformerLayer(nn.Module):\n",
        "    def __init__(self, dim, output_dim, input_resolution, depth, num_heads, win_size,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm,  use_checkpoint=False,\n",
        "                 token_projection='linear',token_mlp='ffn'):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        # build blocks\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            LeWinTransformer_CatCross(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, win_size=win_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else win_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer,token_projection=token_projection,token_mlp=token_mlp)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
        "        \n",
        "    def forward(self, x, attn_kv=None, mask=None):\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x = checkpoint.checkpoint(blk, x)\n",
        "            else:\n",
        "                x = blk(x,attn_kv, mask)\n",
        "        return x \n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops()\n",
        "        return flops\n",
        "\n",
        "class Uformer(nn.Module):\n",
        "    def __init__(self, img_size=128, in_chans=3,\n",
        "                 embed_dim=32, depths=[2, 2, 2, 2, 2, 2, 2, 2, 2], num_heads=[1, 2, 4, 8, 16, 16, 8, 4, 2],\n",
        "                 win_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, token_projection='linear', token_mlp='ffn', se_layer=False,\n",
        "                 dowsample=Downsample, upsample=Upsample, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_enc_layers = len(depths)//2\n",
        "        self.num_dec_layers = len(depths)//2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_projection = token_projection\n",
        "        self.mlp = token_mlp\n",
        "        self.win_size =win_size\n",
        "        self.reso = img_size\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:self.num_enc_layers]))] \n",
        "        conv_dpr = [drop_path_rate]*depths[4]\n",
        "        dec_dpr = enc_dpr[::-1]\n",
        "\n",
        "        # build layers\n",
        "\n",
        "        # Input/Output\n",
        "        self.input_proj = InputProj(in_channel=in_chans, out_channel=embed_dim, kernel_size=3, stride=1, act_layer=nn.LeakyReLU)\n",
        "        self.output_proj = OutputProj(in_channel=2*embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoderlayer_0 = BasicUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[0],\n",
        "                            num_heads=num_heads[0],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.dowsample_0 = dowsample(embed_dim, embed_dim*2)\n",
        "        self.encoderlayer_1 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[1],\n",
        "                            num_heads=num_heads[1],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:1]):sum(depths[:2])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.dowsample_1 = dowsample(embed_dim*2, embed_dim*4)\n",
        "        self.encoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[2],\n",
        "                            num_heads=num_heads[2],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:2]):sum(depths[:3])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.dowsample_2 = dowsample(embed_dim*4, embed_dim*8)\n",
        "        self.encoderlayer_3 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[3],\n",
        "                            num_heads=num_heads[3],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:3]):sum(depths[:4])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.dowsample_3 = dowsample(embed_dim*8, embed_dim*16)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size // (2 ** 4),\n",
        "                                                img_size // (2 ** 4)),\n",
        "                            depth=depths[4],\n",
        "                            num_heads=num_heads[4],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=conv_dpr,\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample_0 = upsample(embed_dim*16, embed_dim*8)\n",
        "        self.decoderlayer_0 = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[5],\n",
        "                            num_heads=num_heads[5],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[:depths[5]],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_1 = upsample(embed_dim*16, embed_dim*4)\n",
        "        self.decoderlayer_1 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[6],\n",
        "                            num_heads=num_heads[6],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:6]):sum(depths[5:7])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_2 = upsample(embed_dim*8, embed_dim*2)\n",
        "        self.decoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[7],\n",
        "                            num_heads=num_heads[7],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:7]):sum(depths[5:8])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_3 = upsample(embed_dim*4, embed_dim)\n",
        "        self.decoderlayer_3 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[8],\n",
        "                            num_heads=num_heads[8],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:8]):sum(depths[5:9])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp},win_size={self.win_size}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input Projection\n",
        "        y = self.input_proj(x)\n",
        "        y = self.pos_drop(y)\n",
        "        #Encoder\n",
        "        conv0 = self.encoderlayer_0(y,mask=mask)\n",
        "        pool0 = self.dowsample_0(conv0)\n",
        "        conv1 = self.encoderlayer_1(pool0,mask=mask)\n",
        "        pool1 = self.dowsample_1(conv1)\n",
        "        conv2 = self.encoderlayer_2(pool1,mask=mask)\n",
        "        pool2 = self.dowsample_2(conv2)\n",
        "        conv3 = self.encoderlayer_3(pool2,mask=mask)\n",
        "        pool3 = self.dowsample_3(conv3)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv4 = self.conv(pool3, mask=mask)\n",
        "\n",
        "        #Decoder\n",
        "        up0 = self.upsample_0(conv4)\n",
        "        deconv0 = torch.cat([up0,conv3],-1)\n",
        "        deconv0 = self.decoderlayer_0(deconv0,mask=mask)\n",
        "        \n",
        "        up1 = self.upsample_1(deconv0)\n",
        "        deconv1 = torch.cat([up1,conv2],-1)\n",
        "        deconv1 = self.decoderlayer_1(deconv1,mask=mask)\n",
        "\n",
        "        up2 = self.upsample_2(deconv1)\n",
        "        deconv2 = torch.cat([up2,conv1],-1)\n",
        "        deconv2 = self.decoderlayer_2(deconv2,mask=mask)\n",
        "\n",
        "        up3 = self.upsample_3(deconv2)\n",
        "        deconv3 = torch.cat([up3,conv0],-1)\n",
        "        deconv3 = self.decoderlayer_3(deconv3,mask=mask)\n",
        "\n",
        "        # Output Projection\n",
        "        y = self.output_proj(deconv3)\n",
        "        return x + y\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        # Input Projection\n",
        "        flops += self.input_proj.flops(self.reso,self.reso)\n",
        "        # Encoder\n",
        "        flops += self.encoderlayer_0.flops()+self.dowsample_0.flops(self.reso,self.reso)\n",
        "        flops += self.encoderlayer_1.flops()+self.dowsample_1.flops(self.reso//2,self.reso//2)\n",
        "        flops += self.encoderlayer_2.flops()+self.dowsample_2.flops(self.reso//2**2,self.reso//2**2)\n",
        "        flops += self.encoderlayer_3.flops()+self.dowsample_3.flops(self.reso//2**3,self.reso//2**3)\n",
        "\n",
        "        # Bottleneck\n",
        "        flops += self.conv.flops()\n",
        "\n",
        "        # Decoder\n",
        "        flops += self.upsample_0.flops(self.reso//2**4,self.reso//2**4)+self.decoderlayer_0.flops()\n",
        "        flops += self.upsample_1.flops(self.reso//2**3,self.reso//2**3)+self.decoderlayer_1.flops()\n",
        "        flops += self.upsample_2.flops(self.reso//2**2,self.reso//2**2)+self.decoderlayer_2.flops()\n",
        "        flops += self.upsample_3.flops(self.reso//2,self.reso//2)+self.decoderlayer_3.flops()\n",
        "        \n",
        "        # Output Projection\n",
        "        flops += self.output_proj.flops(self.reso,self.reso)\n",
        "        return flops\n",
        "\n",
        "class Uformer_Cross(nn.Module):\n",
        "    def __init__(self, img_size=128, in_chans=3,\n",
        "                 embed_dim=32, depths=[2, 2, 2, 2, 2, 2, 2, 2, 2], num_heads=[1, 2, 4, 8, 16, 8, 4, 2, 1],\n",
        "                 win_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, token_projection='linear', token_mlp='ffn', \n",
        "                 dowsample=Downsample, upsample=Upsample, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_enc_layers = len(depths)//2\n",
        "        self.num_dec_layers = len(depths)//2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_projection = token_projection\n",
        "        self.mlp = token_mlp\n",
        "        self.win_size =win_size\n",
        "        self.reso = img_size\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:self.num_enc_layers]))]\n",
        "        conv_dpr = [drop_path_rate]*depths[4]\n",
        "        dec_dpr = enc_dpr[::-1]\n",
        "\n",
        "        # build layers\n",
        "\n",
        "        # Input/Output\n",
        "        self.input_proj = InputProj(in_channel=in_chans, out_channel=embed_dim, kernel_size=3, stride=1, act_layer=nn.LeakyReLU)\n",
        "        self.output_proj = OutputProj(in_channel=embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoderlayer_0 = BasicUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[0],\n",
        "                            num_heads=num_heads[0],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_0 = dowsample(embed_dim, embed_dim*2)\n",
        "        self.encoderlayer_1 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[1],\n",
        "                            num_heads=num_heads[1],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:1]):sum(depths[:2])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_1 = dowsample(embed_dim*2, embed_dim*4)\n",
        "        self.encoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[2],\n",
        "                            num_heads=num_heads[2],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:2]):sum(depths[:3])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_2 = dowsample(embed_dim*4, embed_dim*8)\n",
        "        self.encoderlayer_3 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[3],\n",
        "                            num_heads=num_heads[3],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:3]):sum(depths[:4])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_3 = dowsample(embed_dim*8, embed_dim*16)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size // (2 ** 4),\n",
        "                                                img_size // (2 ** 4)),\n",
        "                            depth=depths[4],\n",
        "                            num_heads=num_heads[4],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=conv_dpr,\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample_0 = upsample(embed_dim*16, embed_dim*8)\n",
        "        self.decoderlayer_0 = CrossUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[5],\n",
        "                            num_heads=num_heads[5],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[:depths[5]],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_1 = upsample(embed_dim*8, embed_dim*4)\n",
        "        self.decoderlayer_1 = CrossUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[6],\n",
        "                            num_heads=num_heads[6],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:6]):sum(depths[5:7])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_2 = upsample(embed_dim*4, embed_dim*2)\n",
        "        self.decoderlayer_2 = CrossUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[7],\n",
        "                            num_heads=num_heads[7],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:7]):sum(depths[5:8])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_3 = upsample(embed_dim*2, embed_dim)\n",
        "        self.decoderlayer_3 = CrossUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[8],\n",
        "                            num_heads=num_heads[8],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:8]):sum(depths[5:9])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp},win_size={self.win_size}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input Projection\n",
        "        y = self.input_proj(x)\n",
        "        y = self.pos_drop(y)\n",
        "\n",
        "        # Encoder\n",
        "        conv0 = self.encoderlayer_0(y, mask=mask)\n",
        "        pool0 = self.dowsample_0(conv0)\n",
        "        conv1 = self.encoderlayer_1(pool0, mask=mask)\n",
        "        pool1 = self.dowsample_1(conv1)\n",
        "        conv2 = self.encoderlayer_2(pool1, mask=mask)\n",
        "        pool2 = self.dowsample_2(conv2)\n",
        "        conv3 = self.encoderlayer_3(pool2, mask=mask)\n",
        "        pool3 = self.dowsample_3(conv3)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv4 = self.conv(pool3, mask=mask)\n",
        "\n",
        "        # Decoder\n",
        "        up0 = self.upsample_0(conv4)\n",
        "        deconv0 = self.decoderlayer_0(up0,attn_kv=conv3, mask=mask)\n",
        "\n",
        "        up1 = self.upsample_1(deconv0)\n",
        "        deconv1 = self.decoderlayer_1(up1,attn_kv=conv2, mask=mask)\n",
        "\n",
        "        up2 = self.upsample_2(deconv1)\n",
        "        deconv2 = self.decoderlayer_2(up2,attn_kv=conv1, mask=mask)\n",
        "\n",
        "        up3 = self.upsample_3(deconv2)\n",
        "        deconv3 = self.decoderlayer_3(up3,attn_kv=conv0, mask=mask)\n",
        "\n",
        "        # Output Projection\n",
        "        y = self.output_proj(deconv3)\n",
        "        return x + y\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        # Input Projection\n",
        "        flops += self.input_proj.flops(self.reso,self.reso)\n",
        "        # Encoder\n",
        "        flops += self.encoderlayer_0.flops()+self.dowsample_0.flops(self.reso,self.reso)\n",
        "        flops += self.encoderlayer_1.flops()+self.dowsample_1.flops(self.reso//2,self.reso//2)\n",
        "        flops += self.encoderlayer_2.flops()+self.dowsample_2.flops(self.reso//2**2,self.reso//2**2)\n",
        "        flops += self.encoderlayer_3.flops()+self.dowsample_3.flops(self.reso//2**3,self.reso//2**3)\n",
        "\n",
        "        # Bottleneck\n",
        "        flops += self.conv.flops()\n",
        "\n",
        "        # Decoder\n",
        "        flops += self.upsample_0.flops(self.reso//2**4,self.reso//2**4)+self.decoderlayer_0.flops()\n",
        "        flops += self.upsample_1.flops(self.reso//2**3,self.reso//2**3)+self.decoderlayer_1.flops()\n",
        "        flops += self.upsample_2.flops(self.reso//2**2,self.reso//2**2)+self.decoderlayer_2.flops()\n",
        "        flops += self.upsample_3.flops(self.reso//2,self.reso//2)+self.decoderlayer_3.flops()\n",
        "        \n",
        "        # Output Projection\n",
        "        flops += self.output_proj.flops(self.reso,self.reso)\n",
        "        return flops\n",
        "        \n",
        "class Uformer_CatCross(nn.Module):\n",
        "    def __init__(self, img_size=128, in_chans=3,\n",
        "                 embed_dim=32, depths=[2, 2, 2, 2, 2, 2, 2, 2, 2], num_heads=[1, 2, 4, 8, 16, 8, 4, 2, 1],\n",
        "                 win_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, token_projection='linear', token_mlp='ffn', \n",
        "                 dowsample=Downsample, upsample=Upsample, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_enc_layers = len(depths)//2\n",
        "        self.num_dec_layers = len(depths)//2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_projection = token_projection\n",
        "        self.mlp = token_mlp\n",
        "        self.win_size =win_size\n",
        "        self.reso = img_size\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:self.num_enc_layers]))]  # stochastic depth decay rule\n",
        "        conv_dpr = [drop_path_rate]*depths[4]\n",
        "        dec_dpr = enc_dpr[::-1]\n",
        "\n",
        "        # build layers\n",
        "\n",
        "        # Input/Output\n",
        "        self.input_proj = InputProj(in_channel=in_chans, out_channel=embed_dim, kernel_size=3, stride=1, act_layer=nn.LeakyReLU)\n",
        "        self.output_proj = OutputProj(in_channel=embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoderlayer_0 = BasicUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[0],\n",
        "                            num_heads=num_heads[0],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_0 = dowsample(embed_dim, embed_dim*2)\n",
        "        self.encoderlayer_1 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[1],\n",
        "                            num_heads=num_heads[1],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:1]):sum(depths[:2])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_1 = dowsample(embed_dim*2, embed_dim*4)\n",
        "        self.encoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[2],\n",
        "                            num_heads=num_heads[2],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:2]):sum(depths[:3])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_2 = dowsample(embed_dim*4, embed_dim*8)\n",
        "        self.encoderlayer_3 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[3],\n",
        "                            num_heads=num_heads[3],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:3]):sum(depths[:4])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.dowsample_3 = dowsample(embed_dim*8, embed_dim*16)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size // (2 ** 4),\n",
        "                                                img_size // (2 ** 4)),\n",
        "                            depth=depths[4],\n",
        "                            num_heads=num_heads[4],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=conv_dpr,\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample_0 = upsample(embed_dim*16, embed_dim*8)\n",
        "        self.decoderlayer_0 = CatCrossUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size // (2 ** 3),\n",
        "                                                img_size // (2 ** 3)),\n",
        "                            depth=depths[5],\n",
        "                            num_heads=num_heads[5],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[:depths[5]],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_1 = upsample(embed_dim*8, embed_dim*4)\n",
        "        self.decoderlayer_1 = CatCrossUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size // (2 ** 2),\n",
        "                                                img_size // (2 ** 2)),\n",
        "                            depth=depths[6],\n",
        "                            num_heads=num_heads[6],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:6]):sum(depths[5:7])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_2 = upsample(embed_dim*4, embed_dim*2)\n",
        "        self.decoderlayer_2 = CatCrossUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size // 2,\n",
        "                                                img_size // 2),\n",
        "                            depth=depths[7],\n",
        "                            num_heads=num_heads[7],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:7]):sum(depths[5:8])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "        self.upsample_3 = upsample(embed_dim*2, embed_dim)\n",
        "        self.decoderlayer_3 = CatCrossUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[8],\n",
        "                            num_heads=num_heads[8],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:8]):sum(depths[5:9])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp},win_size={self.win_size}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input Projection\n",
        "        y = self.input_proj(x)\n",
        "        y = self.pos_drop(y)\n",
        "        # Encoder\n",
        "        conv0 = self.encoderlayer_0(y, mask=mask)\n",
        "        pool0 = self.dowsample_0(conv0)\n",
        "        conv1 = self.encoderlayer_1(pool0, mask=mask)\n",
        "        pool1 = self.dowsample_1(conv1)\n",
        "        conv2 = self.encoderlayer_2(pool1, mask=mask)\n",
        "        pool2 = self.dowsample_2(conv2)\n",
        "        conv3 = self.encoderlayer_3(pool2, mask=mask)\n",
        "        pool3 = self.dowsample_3(conv3)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv4 = self.conv(pool3, mask=mask)\n",
        "\n",
        "        # Decoder\n",
        "        up0 = self.upsample_0(conv4)\n",
        "        deconv0 = self.decoderlayer_0(up0,attn_kv=conv3,mask=mask)\n",
        "\n",
        "        up1 = self.upsample_1(deconv0)\n",
        "        deconv1 = self.decoderlayer_1(up1,attn_kv=conv2,mask=mask)\n",
        "\n",
        "        up2 = self.upsample_2(deconv1)\n",
        "        deconv2 = self.decoderlayer_2(up2,attn_kv=conv1,mask=mask)\n",
        "\n",
        "        up3 = self.upsample_3(deconv2)\n",
        "        deconv3 = self.decoderlayer_3(up3,attn_kv=conv0,mask=mask)\n",
        "\n",
        "        # Output Projection\n",
        "        y = self.output_proj(deconv3)\n",
        "        return x + y\n",
        "\n",
        "    def flops(self):\n",
        "        flops = 0\n",
        "        # Input Projection\n",
        "        flops += self.input_proj.flops(self.reso,self.reso)\n",
        "        # Encoder\n",
        "        flops += self.encoderlayer_0.flops()+self.dowsample_0.flops(self.reso,self.reso)\n",
        "        flops += self.encoderlayer_1.flops()+self.dowsample_1.flops(self.reso//2,self.reso//2)\n",
        "        flops += self.encoderlayer_2.flops()+self.dowsample_2.flops(self.reso//2**2,self.reso//2**2)\n",
        "        flops += self.encoderlayer_3.flops()+self.dowsample_3.flops(self.reso//2**3,self.reso//2**3)\n",
        "\n",
        "        # Bottleneck\n",
        "        flops += self.conv.flops()\n",
        "\n",
        "        # Decoder\n",
        "        flops += self.upsample_0.flops(self.reso//2**4,self.reso//2**4)+self.decoderlayer_0.flops()\n",
        "        flops += self.upsample_1.flops(self.reso//2**3,self.reso//2**3)+self.decoderlayer_1.flops()\n",
        "        flops += self.upsample_2.flops(self.reso//2**2,self.reso//2**2)+self.decoderlayer_2.flops()\n",
        "        flops += self.upsample_3.flops(self.reso//2,self.reso//2)+self.decoderlayer_3.flops()\n",
        "        \n",
        "        # Output Projection\n",
        "        flops += self.output_proj.flops(self.reso,self.reso)\n",
        "        return flops\n",
        "# class LeWinformer(nn.Module):\n",
        "#     def __init__(self, img_size=128, in_chans=3,\n",
        "#                  embed_dim=32, depth=12,\n",
        "#                  win_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "#                  drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "#                  norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "#                  use_checkpoint=False, token_projection='linear', token_mlp='ffn', se_layer=False,**kwargs):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.transformer_layers = nn.ModuleList([])\n",
        "#         self.embed_dim = embed_dim\n",
        "#         self.num_heads = embed_dim//32 or 1\n",
        "#         self.patch_norm = patch_norm\n",
        "#         self.mlp_ratio = mlp_ratio\n",
        "#         self.token_projection = token_projection\n",
        "#         self.mlp = token_mlp\n",
        "#         self.win_size =win_size\n",
        "        \n",
        "#         self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "#         # stochastic depth\n",
        "#         dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] \n",
        "\n",
        "#         # build layers\n",
        "\n",
        "#         # Input/Output\n",
        "#         self.input_proj = InputProj(in_channel=in_chans, out_channel=embed_dim, kernel_size=3, stride=1, act_layer=nn.LeakyReLU)\n",
        "#         self.output_proj = OutputProj(in_channel=embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "        \n",
        "#         # LeWin Transformer\n",
        "#         for i in range(depth):\n",
        "#             dim = embed_dim\n",
        "#             self.transformer_layers.append(nn.ModuleList([BasicUformerLayer(dim=dim,\n",
        "#                             output_dim=embed_dim,\n",
        "#                             input_resolution=(img_size,\n",
        "#                                                 img_size),\n",
        "#                             depth=1,\n",
        "#                             num_heads=self.num_heads,\n",
        "#                             win_size=win_size,\n",
        "#                             mlp_ratio=self.mlp_ratio,\n",
        "#                             qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "#                             drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "#                             drop_path=dpr[i],\n",
        "#                             norm_layer=norm_layer,\n",
        "#                             use_checkpoint=use_checkpoint,\n",
        "#                             token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer),\n",
        "#                 Downsample()\n",
        "#             ]))\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "#     @torch.jit.ignore\n",
        "#     def no_weight_decay(self):\n",
        "#         return {'absolute_pos_embed'}\n",
        "\n",
        "#     @torch.jit.ignore\n",
        "#     def no_weight_decay_keywords(self):\n",
        "#         return {'relative_position_bias_table'}\n",
        "\n",
        "#     def extra_repr(self) -> str:\n",
        "#         return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp},win_size={self.win_size}\"\n",
        "\n",
        "#     def forward(self, x, mask=None):\n",
        "#         # Input Projection\n",
        "#         y = self.input_proj(x)\n",
        "#         y = self.pos_drop(y)\n",
        "#         #Encoder\n",
        "#         for lewin in self.transformer_layers:\n",
        "#             y = lewin(y)\n",
        "\n",
        "#         # Output Projection\n",
        "#         y = self.output_proj(y)\n",
        "#         return x + y\n",
        "\n",
        "### single-scale Uformer is computationally too costly.\n",
        "class Uformer_singlescale(nn.Module):\n",
        "    def __init__(self, img_size=128, in_chans=3,\n",
        "                 embed_dim=32, depths=[2, 2, 2, 2, 2, 2, 2, 2, 2], num_heads=[1, 2, 4, 8, 16, 16, 8, 4, 2],\n",
        "                 win_size=8, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
        "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
        "                 norm_layer=nn.LayerNorm, patch_norm=True,\n",
        "                 use_checkpoint=False, token_projection='linear', token_mlp='ffn', se_layer=False,\n",
        "                 downsample=Downsample, upsample=Upsample, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_enc_layers = len(depths)//2\n",
        "        self.num_dec_layers = len(depths)//2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_norm = patch_norm\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.token_projection = token_projection\n",
        "        self.mlp = token_mlp\n",
        "        self.win_size =win_size\n",
        "        \n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth\n",
        "        enc_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths[:self.num_enc_layers]))] \n",
        "        conv_dpr = [drop_path_rate]*depths[4]\n",
        "        dec_dpr = enc_dpr[::-1]\n",
        "\n",
        "        # build layers\n",
        "\n",
        "        # Input/Output\n",
        "        self.input_proj = InputProj(in_channel=in_chans, out_channel=embed_dim, kernel_size=3, stride=1, act_layer=nn.LeakyReLU)\n",
        "        self.output_proj = OutputProj(in_channel=2*embed_dim, out_channel=in_chans, kernel_size=3, stride=1)\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoderlayer_0 = BasicUformerLayer(dim=embed_dim,\n",
        "                            output_dim=embed_dim,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[0],\n",
        "                            num_heads=num_heads[0],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:0]):sum(depths[:1])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.downsample_0 = downsample(embed_dim, embed_dim*2,downsample=False)\n",
        "        self.encoderlayer_1 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[1],\n",
        "                            num_heads=num_heads[1],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:1]):sum(depths[:2])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.downsample_1 = downsample(embed_dim*2, embed_dim*4,downsample=False)\n",
        "        self.encoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[2],\n",
        "                            num_heads=num_heads[2],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:2]):sum(depths[:3])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.downsample_2 = downsample(embed_dim*4, embed_dim*8,downsample=False)\n",
        "        self.encoderlayer_3 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[3],\n",
        "                            num_heads=num_heads[3],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=enc_dpr[sum(depths[:3]):sum(depths[:4])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.downsample_3 = downsample(embed_dim*8, embed_dim*16,downsample=False)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.conv = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[4],\n",
        "                            num_heads=num_heads[4],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=conv_dpr,\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "\n",
        "        # Decoder\n",
        "        self.upsample_0 = upsample(embed_dim*16, embed_dim*8,upsample=False)\n",
        "        self.decoderlayer_0 = BasicUformerLayer(dim=embed_dim*16,\n",
        "                            output_dim=embed_dim*16,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[5],\n",
        "                            num_heads=num_heads[5],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[:depths[5]],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_1 = upsample(embed_dim*16, embed_dim*4,upsample=False)\n",
        "        self.decoderlayer_1 = BasicUformerLayer(dim=embed_dim*8,\n",
        "                            output_dim=embed_dim*8,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[6],\n",
        "                            num_heads=num_heads[6],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:6]):sum(depths[5:7])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_2 = upsample(embed_dim*8, embed_dim*2,upsample=False)\n",
        "        self.decoderlayer_2 = BasicUformerLayer(dim=embed_dim*4,\n",
        "                            output_dim=embed_dim*4,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[7],\n",
        "                            num_heads=num_heads[7],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:7]):sum(depths[5:8])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "        self.upsample_3 = upsample(embed_dim*4, embed_dim,upsample=False)\n",
        "        self.decoderlayer_3 = BasicUformerLayer(dim=embed_dim*2,\n",
        "                            output_dim=embed_dim*2,\n",
        "                            input_resolution=(img_size,\n",
        "                                                img_size),\n",
        "                            depth=depths[8],\n",
        "                            num_heads=num_heads[8],\n",
        "                            win_size=win_size,\n",
        "                            mlp_ratio=self.mlp_ratio,\n",
        "                            qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                            drop=drop_rate, attn_drop=attn_drop_rate,\n",
        "                            drop_path=dec_dpr[sum(depths[5:8]):sum(depths[5:9])],\n",
        "                            norm_layer=norm_layer,\n",
        "                            use_checkpoint=use_checkpoint,\n",
        "                            token_projection=token_projection,token_mlp=token_mlp,se_layer=se_layer)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'absolute_pos_embed'}\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay_keywords(self):\n",
        "        return {'relative_position_bias_table'}\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"embed_dim={self.embed_dim}, token_projection={self.token_projection}, token_mlp={self.mlp},win_size={self.win_size}\"\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Input Projection\n",
        "        y = self.input_proj(x)\n",
        "        y = self.pos_drop(y)\n",
        "        #Encoder\n",
        "        conv0 = self.encoderlayer_0(y,mask=mask)\n",
        "        pool0 = self.downsample_0(conv0)\n",
        "        conv1 = self.encoderlayer_1(pool0,mask=mask)\n",
        "        pool1 = self.downsample_1(conv1)\n",
        "        conv2 = self.encoderlayer_2(pool1,mask=mask)\n",
        "        pool2 = self.downsample_2(conv2)\n",
        "        conv3 = self.encoderlayer_3(pool2,mask=mask)\n",
        "        pool3 = self.downsample_3(conv3)\n",
        "\n",
        "        # Bottleneck\n",
        "        conv4 = self.conv(pool3, mask=mask)\n",
        "\n",
        "        #Decoder\n",
        "        up0 = self.upsample_0(conv4)\n",
        "        deconv0 = torch.cat([up0,conv3],-1)\n",
        "        deconv0 = self.decoderlayer_0(deconv0,mask=mask)\n",
        "        \n",
        "        up1 = self.upsample_1(deconv0)\n",
        "        deconv1 = torch.cat([up1,conv2],-1)\n",
        "        deconv1 = self.decoderlayer_1(deconv1,mask=mask)\n",
        "\n",
        "        up2 = self.upsample_2(deconv1)\n",
        "        deconv2 = torch.cat([up2,conv1],-1)\n",
        "        deconv2 = self.decoderlayer_2(deconv2,mask=mask)\n",
        "\n",
        "        up3 = self.upsample_3(deconv2)\n",
        "        deconv3 = torch.cat([up3,conv0],-1)\n",
        "        deconv3 = self.decoderlayer_3(deconv3,mask=mask)\n",
        "\n",
        "        # Output Projection\n",
        "        y = self.output_proj(deconv3)\n",
        "        return x + y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    arch = Uformer\n",
        "    input_size = 256\n",
        "    # arch = Uformer_Cross\n",
        "    depths=[2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
        "    # model_restoration = UNet(dim=32)\n",
        "    model_restoration = arch(img_size=input_size, embed_dim=44,depths=depths,\n",
        "                 win_size=8, mlp_ratio=4., qkv_bias=True,\n",
        "                 token_projection='linear', token_mlp='leff',\n",
        "                 downsample=Downsample, upsample=Upsample,se_layer=False)\n",
        "    # arch = LeWinformer    \n",
        "    # depth = 20\n",
        "    # model_restoration = arch(embed_dim=16,depth=depth,\n",
        "    #              win_size=8, mlp_ratio=4., qkv_bias=True,\n",
        "    #              token_projection='linear', token_mlp='leff',se_layer=False)         \n",
        "    # from ptflops import get_model_complexity_info\n",
        "    # macs, params = get_model_complexity_info(model_restoration, (3, input_size, input_size), as_strings=True,\n",
        "    #                                             print_per_layer_stat=True, verbose=True)\n",
        "    # print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "    # print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "    # print(\"number of GFLOPs: %.2f G\"%(model_restoration.flops(input_size,input_size) / 1e9))\n",
        "    print(\"number of GFLOPs: %.2f G\"%(model_restoration.flops() / 1e9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ndjG7rUYre"
      },
      "source": [
        "# ðŸ”¨ Funciones Ãºtiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v3I6jPlkNLA"
      },
      "source": [
        "### Antialias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjuTCs7hkOhf"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2019, Adobe Inc. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike\n",
        "# 4.0 International Public License. To view a copy of this license, visit\n",
        "# https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode.\n",
        "\n",
        "\n",
        "\n",
        "########  https://github.com/adobe/antialiased-cnns/blob/master/models_lpf/__init__.py\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2)), int(1.*(filt_size-1)/2), int(np.ceil(1.*(filt_size-1)/2))]\n",
        "        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride-1)/2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        # print('Filter size [%i]'%filt_size)\n",
        "        if(self.filt_size==1):\n",
        "            a = np.array([1.,])\n",
        "        elif(self.filt_size==2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size==3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size==4):    \n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size==5):    \n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size==6):    \n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size==7):    \n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a[:,None]*a[None,:])\n",
        "        filt = filt/torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n",
        "\n",
        "        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size==1):\n",
        "            if(self.pad_off==0):\n",
        "                return inp[:,:,::self.stride,::self.stride]    \n",
        "            else:\n",
        "                return self.pad(inp)[:,:,::self.stride,::self.stride]\n",
        "        else:\n",
        "            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "def get_pad_layer(pad_type):\n",
        "    if(pad_type in ['refl','reflect']):\n",
        "        PadLayer = nn.ReflectionPad2d\n",
        "    elif(pad_type in ['repl','replicate']):\n",
        "        PadLayer = nn.ReplicationPad2d\n",
        "    elif(pad_type=='zero'):\n",
        "        PadLayer = nn.ZeroPad2d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized'%pad_type)\n",
        "    return PadLayer\n",
        "\n",
        "\n",
        "class Downsample1D(nn.Module):\n",
        "    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n",
        "        super(Downsample1D, self).__init__()\n",
        "        self.filt_size = filt_size\n",
        "        self.pad_off = pad_off\n",
        "        self.pad_sizes = [int(1. * (filt_size - 1) / 2), int(np.ceil(1. * (filt_size - 1) / 2))]\n",
        "        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n",
        "        self.stride = stride\n",
        "        self.off = int((self.stride - 1) / 2.)\n",
        "        self.channels = channels\n",
        "\n",
        "        # print('Filter size [%i]' % filt_size)\n",
        "        if(self.filt_size == 1):\n",
        "            a = np.array([1., ])\n",
        "        elif(self.filt_size == 2):\n",
        "            a = np.array([1., 1.])\n",
        "        elif(self.filt_size == 3):\n",
        "            a = np.array([1., 2., 1.])\n",
        "        elif(self.filt_size == 4):\n",
        "            a = np.array([1., 3., 3., 1.])\n",
        "        elif(self.filt_size == 5):\n",
        "            a = np.array([1., 4., 6., 4., 1.])\n",
        "        elif(self.filt_size == 6):\n",
        "            a = np.array([1., 5., 10., 10., 5., 1.])\n",
        "        elif(self.filt_size == 7):\n",
        "            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n",
        "\n",
        "        filt = torch.Tensor(a)\n",
        "        filt = filt / torch.sum(filt)\n",
        "        self.register_buffer('filt', filt[None, None, :].repeat((self.channels, 1, 1)))\n",
        "\n",
        "        self.pad = get_pad_layer_1d(pad_type)(self.pad_sizes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        if(self.filt_size == 1):\n",
        "            if(self.pad_off == 0):\n",
        "                return inp[:, :, ::self.stride]\n",
        "            else:\n",
        "                return self.pad(inp)[:, :, ::self.stride]\n",
        "        else:\n",
        "            return F.conv1d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n",
        "\n",
        "\n",
        "def get_pad_layer_1d(pad_type):\n",
        "    if(pad_type in ['refl', 'reflect']):\n",
        "        PadLayer = nn.ReflectionPad1d\n",
        "    elif(pad_type in ['repl', 'replicate']):\n",
        "        PadLayer = nn.ReplicationPad1d\n",
        "    elif(pad_type == 'zero'):\n",
        "        PadLayer = nn.ZeroPad1d\n",
        "    else:\n",
        "        print('Pad type [%s] not recognized' % pad_type)\n",
        "    return PadLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9KbOV2YkQYX"
      },
      "source": [
        "### Bundle Submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVxkU_lQkScM"
      },
      "outputs": [],
      "source": [
        " # Author: Tobias PlÃ¶tz, TU Darmstadt (tobias.ploetz@visinf.tu-darmstadt.de)\n",
        "\n",
        " # This file is part of the implementation as described in the CVPR 2017 paper:\n",
        " # Tobias PlÃ¶tz and Stefan Roth, Benchmarking Denoising Algorithms with Real Photographs.\n",
        " # Please see the file LICENSE.txt for the license governing this code.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "def bundle_submissions_raw(submission_folder,session):\n",
        "    '''\n",
        "    Bundles submission data for raw denoising\n",
        "    submission_folder Folder where denoised images reside\n",
        "    Output is written to <submission_folder>/bundled/. Please submit\n",
        "    the content of this folder.\n",
        "    '''\n",
        "\n",
        "    out_folder = os.path.join(submission_folder, session)\n",
        "    # out_folder = os.path.join(submission_folder, \"bundled/\")\n",
        "    try:\n",
        "        os.mkdir(out_folder)\n",
        "    except:pass\n",
        "\n",
        "    israw = True\n",
        "    eval_version=\"1.0\"\n",
        "\n",
        "    for i in range(50):\n",
        "        Idenoised = np.zeros((20,), dtype=np.object)\n",
        "        for bb in range(20):\n",
        "            filename = '%04d_%02d.mat'%(i+1,bb+1)\n",
        "            s = sio.loadmat(os.path.join(submission_folder,filename))\n",
        "            Idenoised_crop = s[\"Idenoised_crop\"]\n",
        "            Idenoised[bb] = Idenoised_crop\n",
        "        filename = '%04d.mat'%(i+1)\n",
        "        sio.savemat(os.path.join(out_folder, filename),\n",
        "                    {\"Idenoised\": Idenoised,\n",
        "                     \"israw\": israw,\n",
        "                     \"eval_version\": eval_version},\n",
        "                    )\n",
        "\n",
        "def bundle_submissions_srgb(submission_folder,session):\n",
        "    '''\n",
        "    Bundles submission data for sRGB denoising\n",
        "    \n",
        "    submission_folder Folder where denoised images reside\n",
        "    Output is written to <submission_folder>/bundled/. Please submit\n",
        "    the content of this folder.\n",
        "    '''\n",
        "    out_folder = os.path.join(submission_folder, session)\n",
        "    # out_folder = os.path.join(submission_folder, \"bundled/\")\n",
        "    try:\n",
        "        os.mkdir(out_folder)\n",
        "    except:pass\n",
        "    israw = False\n",
        "    eval_version=\"1.0\"\n",
        "\n",
        "    for i in range(50):\n",
        "        Idenoised = np.zeros((20,), dtype=np.object)\n",
        "        for bb in range(20):\n",
        "            filename = '%04d_%02d.mat'%(i+1,bb+1)\n",
        "            s = sio.loadmat(os.path.join(submission_folder,filename))\n",
        "            Idenoised_crop = s[\"Idenoised_crop\"]\n",
        "            Idenoised[bb] = Idenoised_crop\n",
        "        filename = '%04d.mat'%(i+1)\n",
        "        sio.savemat(os.path.join(out_folder, filename),\n",
        "                    {\"Idenoised\": Idenoised,\n",
        "                     \"israw\": israw,\n",
        "                     \"eval_version\": eval_version},\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "def bundle_submissions_srgb_v1(submission_folder,session):\n",
        "    '''\n",
        "    Bundles submission data for sRGB denoising\n",
        "    \n",
        "    submission_folder Folder where denoised images reside\n",
        "    Output is written to <submission_folder>/bundled/. Please submit\n",
        "    the content of this folder.\n",
        "    '''\n",
        "    out_folder = os.path.join(submission_folder, session)\n",
        "    # out_folder = os.path.join(submission_folder, \"bundled/\")\n",
        "    try:\n",
        "        os.mkdir(out_folder)\n",
        "    except:pass\n",
        "    israw = False\n",
        "    eval_version=\"1.0\"\n",
        "\n",
        "    for i in range(50):\n",
        "        Idenoised = np.zeros((20,), dtype=np.object)\n",
        "        for bb in range(20):\n",
        "            filename = '%04d_%d.mat'%(i+1,bb+1)\n",
        "            s = sio.loadmat(os.path.join(submission_folder,filename))\n",
        "            Idenoised_crop = s[\"Idenoised_crop\"]\n",
        "            Idenoised[bb] = Idenoised_crop\n",
        "        filename = '%04d.mat'%(i+1)\n",
        "        sio.savemat(os.path.join(out_folder, filename),\n",
        "                    {\"Idenoised\": Idenoised,\n",
        "                     \"israw\": israw,\n",
        "                     \"eval_version\": eval_version},\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRoUNxOtkU5P"
      },
      "source": [
        "### Dataset utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F2qx27ukWzc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "### rotate and flip\n",
        "class Augment_RGB_torch:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def transform0(self, torch_tensor):\n",
        "        return torch_tensor   \n",
        "    def transform1(self, torch_tensor):\n",
        "        torch_tensor = torch.rot90(torch_tensor, k=1, dims=[-1,-2])\n",
        "        return torch_tensor\n",
        "    def transform2(self, torch_tensor):\n",
        "        torch_tensor = torch.rot90(torch_tensor, k=2, dims=[-1,-2])\n",
        "        return torch_tensor\n",
        "    def transform3(self, torch_tensor):\n",
        "        torch_tensor = torch.rot90(torch_tensor, k=3, dims=[-1,-2])\n",
        "        return torch_tensor\n",
        "    def transform4(self, torch_tensor):\n",
        "        torch_tensor = torch_tensor.flip(-2)\n",
        "        return torch_tensor\n",
        "    def transform5(self, torch_tensor):\n",
        "        torch_tensor = (torch.rot90(torch_tensor, k=1, dims=[-1,-2])).flip(-2)\n",
        "        return torch_tensor\n",
        "    def transform6(self, torch_tensor):\n",
        "        torch_tensor = (torch.rot90(torch_tensor, k=2, dims=[-1,-2])).flip(-2)\n",
        "        return torch_tensor\n",
        "    def transform7(self, torch_tensor):\n",
        "        torch_tensor = (torch.rot90(torch_tensor, k=3, dims=[-1,-2])).flip(-2)\n",
        "        return torch_tensor\n",
        "\n",
        "\n",
        "### mix two images\n",
        "class MixUp_AUG:\n",
        "    def __init__(self):\n",
        "        self.dist = torch.distributions.beta.Beta(torch.tensor([1.2]), torch.tensor([1.2]))\n",
        "\n",
        "    def aug(self, rgb_gt, rgb_noisy):\n",
        "        bs = rgb_gt.size(0)\n",
        "        indices = torch.randperm(bs)\n",
        "        rgb_gt2 = rgb_gt[indices]\n",
        "        rgb_noisy2 = rgb_noisy[indices]\n",
        "\n",
        "        lam = self.dist.rsample((bs,1)).view(-1,1,1,1).cuda()\n",
        "\n",
        "        rgb_gt    = lam * rgb_gt + (1-lam) * rgb_gt2\n",
        "        rgb_noisy = lam * rgb_noisy + (1-lam) * rgb_noisy2\n",
        "\n",
        "        return rgb_gt, rgb_noisy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrX_M0qHka_m"
      },
      "source": [
        "### Dir utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6kvrgOZkcYq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)\n",
        "\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def get_last_path(path, session):\n",
        "\tx = natsorted(glob(os.path.join(path,'*%s'%session)))[-1]\n",
        "\treturn x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoK-W1omkewr"
      },
      "source": [
        "### Image utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIZVQ5OKkahd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pickle\n",
        "import cv2\n",
        "\n",
        "def is_numpy_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".npy\"])\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".jpg\"])\n",
        "\n",
        "def is_png_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".png\"])\n",
        "\n",
        "def is_pkl_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in [\".pkl\"])\n",
        "\n",
        "def load_pkl(filename_):\n",
        "    with open(filename_, 'rb') as f:\n",
        "        ret_dict = pickle.load(f)\n",
        "    return ret_dict    \n",
        "\n",
        "def save_dict(dict_, filename_):\n",
        "    with open(filename_, 'wb') as f:\n",
        "        pickle.dump(dict_, f)    \n",
        "\n",
        "def load_npy(filepath):\n",
        "    img = np.load(filepath)\n",
        "    return img\n",
        "\n",
        "def load_img(filepath):\n",
        "    img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32)\n",
        "    img = img/255.\n",
        "    return img\n",
        "\n",
        "def save_img(filepath, img):\n",
        "    cv2.imwrite(filepath,cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "def myPSNR(tar_img, prd_img):\n",
        "    imdff = torch.clamp(prd_img,0,1) - torch.clamp(tar_img,0,1)\n",
        "    rmse = (imdff**2).mean().sqrt()\n",
        "    ps = 20*torch.log10(1/rmse)\n",
        "    return ps\n",
        "\n",
        "def batch_PSNR(img1, img2, average=True):\n",
        "    PSNR = []\n",
        "    for im1, im2 in zip(img1, img2):\n",
        "        psnr = myPSNR(im1, im2)\n",
        "        PSNR.append(psnr)\n",
        "    return sum(PSNR)/len(PSNR) if average else sum(PSNR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbd-MbyKkZlu"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf26q2-7kcBE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "augment   = Augment_RGB_torch()\n",
        "transforms_aug = [method for method in dir(augment) if callable(getattr(augment, method)) if not method.startswith('_')] \n",
        "\n",
        "##################################################################################################\n",
        "class DataLoaderTrain(Dataset):\n",
        "    def __init__(self, rgb_dir, img_options=None, target_transform=None):\n",
        "        super(DataLoaderTrain, self).__init__()\n",
        "\n",
        "        self.target_transform = target_transform\n",
        "        \n",
        "        gt_dir = 'groundtruth'\n",
        "        input_dir = 'input'\n",
        "        \n",
        "        clean_files = sorted(os.listdir(os.path.join(rgb_dir, gt_dir)))\n",
        "        noisy_files = sorted(os.listdir(os.path.join(rgb_dir, input_dir)))\n",
        "        \n",
        "        self.clean_filenames = [os.path.join(rgb_dir, gt_dir, x) for x in clean_files if is_png_file(x)]\n",
        "        self.noisy_filenames = [os.path.join(rgb_dir, input_dir, x)       for x in noisy_files if is_png_file(x)]\n",
        "        \n",
        "        self.img_options=img_options\n",
        "\n",
        "        self.tar_size = len(self.clean_filenames)  # get the size of target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tar_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tar_index   = index % self.tar_size\n",
        "        clean = torch.from_numpy(np.float32(load_img(self.clean_filenames[tar_index])))\n",
        "        noisy = torch.from_numpy(np.float32(load_img(self.noisy_filenames[tar_index])))\n",
        "        \n",
        "        clean = clean.permute(2,0,1)\n",
        "        noisy = noisy.permute(2,0,1)\n",
        "\n",
        "        clean_filename = os.path.split(self.clean_filenames[tar_index])[-1]\n",
        "        noisy_filename = os.path.split(self.noisy_filenames[tar_index])[-1]\n",
        "\n",
        "        #Crop Input and Target\n",
        "        ps = self.img_options['patch_size']\n",
        "        H = clean.shape[1]\n",
        "        W = clean.shape[2]\n",
        "        # r = np.random.randint(0, H - ps) if not H-ps else 0\n",
        "        # c = np.random.randint(0, W - ps) if not H-ps else 0\n",
        "        if H-ps==0:\n",
        "            r=0\n",
        "            c=0\n",
        "        else:\n",
        "            r = np.random.randint(0, H - ps)\n",
        "            c = np.random.randint(0, W - ps)\n",
        "        clean = clean[:, r:r + ps, c:c + ps]\n",
        "        noisy = noisy[:, r:r + ps, c:c + ps]\n",
        "\n",
        "        apply_trans = transforms_aug[random.getrandbits(3)]\n",
        "\n",
        "        clean = getattr(augment, apply_trans)(clean)\n",
        "        noisy = getattr(augment, apply_trans)(noisy)        \n",
        "\n",
        "        return clean, noisy, clean_filename, noisy_filename\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "class DataLoaderTrain_Gaussian(Dataset):\n",
        "    def __init__(self, rgb_dir, noiselevel=5, img_options=None, target_transform=None):\n",
        "        super(DataLoaderTrain_Gaussian, self).__init__()\n",
        "\n",
        "        self.target_transform = target_transform\n",
        "        #pdb.set_trace()\n",
        "        clean_files = sorted(os.listdir(rgb_dir))\n",
        "        #noisy_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
        "        #clean_files = clean_files[0:83000]\n",
        "        #noisy_files = noisy_files[0:83000]\n",
        "        self.clean_filenames = [os.path.join(rgb_dir, x) for x in clean_files if is_png_file(x)]\n",
        "        #self.noisy_filenames = [os.path.join(rgb_dir, 'input', x)       for x in noisy_files if is_png_file(x)]\n",
        "        self.noiselevel = noiselevel\n",
        "        self.img_options=img_options\n",
        "\n",
        "        self.tar_size = len(self.clean_filenames)  # get the size of target\n",
        "        print(self.tar_size)\n",
        "    def __len__(self):\n",
        "        return self.tar_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tar_index   = index % self.tar_size\n",
        "        #print(self.clean_filenames[tar_index])\n",
        "        clean = np.float32(load_img(self.clean_filenames[tar_index]))\n",
        "        #noisy = torch.from_numpy(np.float32(load_img(self.noisy_filenames[tar_index])))\n",
        "        # noiselevel = random.randint(5,20)\n",
        "        noisy = clean + np.float32(np.random.normal(0, self.noiselevel, np.array(clean).shape)/255.)\n",
        "        noisy = np.clip(noisy,0.,1.)\n",
        "        \n",
        "        clean = torch.from_numpy(clean)\n",
        "        noisy = torch.from_numpy(noisy)\n",
        "\n",
        "        clean = clean.permute(2,0,1)\n",
        "        noisy = noisy.permute(2,0,1)\n",
        "\n",
        "        clean_filename = os.path.split(self.clean_filenames[tar_index])[-1]\n",
        "        noisy_filename = os.path.split(self.clean_filenames[tar_index])[-1]\n",
        "\n",
        "        #Crop Input and Target\n",
        "        ps = self.img_options['patch_size']\n",
        "        H = clean.shape[1]\n",
        "        W = clean.shape[2]\n",
        "        r = np.random.randint(0, H - ps)\n",
        "        c = np.random.randint(0, W - ps)\n",
        "        clean = clean[:, r:r + ps, c:c + ps]\n",
        "        noisy = noisy[:, r:r + ps, c:c + ps]\n",
        "\n",
        "        apply_trans = transforms_aug[random.getrandbits(3)]\n",
        "\n",
        "        clean = getattr(augment, apply_trans)(clean)\n",
        "        noisy = getattr(augment, apply_trans)(noisy)\n",
        "\n",
        "        return clean, noisy, clean_filename, noisy_filename\n",
        "##################################################################################################\n",
        "class DataLoaderVal(Dataset):\n",
        "    def __init__(self, rgb_dir, target_transform=None):\n",
        "        super(DataLoaderVal, self).__init__()\n",
        "\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        gt_dir = 'groundtruth'\n",
        "        input_dir = 'input'\n",
        "        \n",
        "        clean_files = sorted(os.listdir(os.path.join(rgb_dir, gt_dir)))\n",
        "        noisy_files = sorted(os.listdir(os.path.join(rgb_dir, input_dir)))\n",
        "\n",
        "\n",
        "        self.clean_filenames = [os.path.join(rgb_dir, gt_dir, x) for x in clean_files if is_png_file(x)]\n",
        "        self.noisy_filenames = [os.path.join(rgb_dir, input_dir, x) for x in noisy_files if is_png_file(x)]\n",
        "        \n",
        "\n",
        "        self.tar_size = len(self.clean_filenames)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tar_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tar_index   = index % self.tar_size\n",
        "        \n",
        "\n",
        "        clean = torch.from_numpy(np.float32(load_img(self.clean_filenames[tar_index])))\n",
        "        noisy = torch.from_numpy(np.float32(load_img(self.noisy_filenames[tar_index])))\n",
        "                \n",
        "        clean_filename = os.path.split(self.clean_filenames[tar_index])[-1]\n",
        "        noisy_filename = os.path.split(self.noisy_filenames[tar_index])[-1]\n",
        "\n",
        "        clean = clean.permute(2,0,1)\n",
        "        noisy = noisy.permute(2,0,1)\n",
        "\n",
        "        return clean, noisy, clean_filename, noisy_filename\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "class DataLoaderTest(Dataset):\n",
        "    def __init__(self, rgb_dir, target_transform=None):\n",
        "        super(DataLoaderTest, self).__init__()\n",
        "\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        noisy_files = sorted(os.listdir(os.path.join(rgb_dir, 'input')))\n",
        "\n",
        "\n",
        "        self.noisy_filenames = [os.path.join(rgb_dir, 'input', x) for x in noisy_files if is_png_file(x)]\n",
        "        \n",
        "\n",
        "        self.tar_size = len(self.noisy_filenames)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tar_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tar_index   = index % self.tar_size\n",
        "        \n",
        "\n",
        "        noisy = torch.from_numpy(np.float32(load_img(self.noisy_filenames[tar_index])))\n",
        "                \n",
        "        noisy_filename = os.path.split(self.noisy_filenames[tar_index])[-1]\n",
        "\n",
        "        noisy = noisy.permute(2,0,1)\n",
        "\n",
        "        return noisy, noisy_filename\n",
        "\n",
        "\n",
        "##################################################################################################\n",
        "\n",
        "class DataLoaderTestSR(Dataset):\n",
        "    def __init__(self, rgb_dir, target_transform=None):\n",
        "        super(DataLoaderTestSR, self).__init__()\n",
        "\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        LR_files = sorted(os.listdir(os.path.join(rgb_dir)))\n",
        "\n",
        "\n",
        "        self.LR_filenames = [os.path.join(rgb_dir, x) for x in LR_files if is_png_file(x)]\n",
        "        \n",
        "\n",
        "        self.tar_size = len(self.LR_filenames)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tar_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tar_index   = index % self.tar_size\n",
        "        \n",
        "\n",
        "        LR = torch.from_numpy(np.float32(load_img(self.LR_filenames[tar_index])))\n",
        "                \n",
        "        LR_filename = os.path.split(self.LR_filenames[tar_index])[-1]\n",
        "\n",
        "        LR = LR.permute(2,0,1)\n",
        "\n",
        "        return LR, LR_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxiOkI49kiVs"
      },
      "source": [
        "### Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA_qth-IkjlH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_training_data(rgb_dir, img_options):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderTrain(rgb_dir, img_options, None)\n",
        "\n",
        "def get_validation_data(rgb_dir):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderVal(rgb_dir, None)\n",
        "\n",
        "\n",
        "def get_test_data(rgb_dir):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderTest(rgb_dir, None)\n",
        "\n",
        "\n",
        "def get_test_data_SR(rgb_dir):\n",
        "    assert os.path.exists(rgb_dir)\n",
        "    return DataLoaderTestSR(rgb_dir, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq8hUNBgkmuR"
      },
      "source": [
        "### Model utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjGgEZCxkoHS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "def freeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad=False\n",
        "\n",
        "def unfreeze(model):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad=True\n",
        "\n",
        "def is_frozen(model):\n",
        "    x = [p.requires_grad for p in model.parameters()]\n",
        "    return not all(x)\n",
        "\n",
        "def save_checkpoint(model_dir, state, session):\n",
        "    epoch = state['epoch']\n",
        "    model_out_path = os.path.join(model_dir,\"model_epoch_{}_{}.pth\".format(epoch,session))\n",
        "    torch.save(state, model_out_path)\n",
        "\n",
        "def load_checkpoint(model, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    try:\n",
        "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    except:\n",
        "        state_dict = checkpoint[\"state_dict\"]\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in state_dict.items():\n",
        "            name = k[7:] if 'module.' in k else k\n",
        "            new_state_dict[name] = v\n",
        "        model.load_state_dict(new_state_dict)\n",
        "\n",
        "\n",
        "def load_checkpoint_multigpu(model, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    state_dict = checkpoint[\"state_dict\"]\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = k[7:] \n",
        "        new_state_dict[name] = v\n",
        "    model.load_state_dict(new_state_dict)\n",
        "\n",
        "def load_start_epoch(weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    epoch = checkpoint[\"epoch\"]\n",
        "    return epoch\n",
        "\n",
        "def load_optim(optimizer, weights):\n",
        "    checkpoint = torch.load(weights)\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    for p in optimizer.param_groups: lr = p['lr']\n",
        "    return lr\n",
        "\n",
        "def get_arch(opt):\n",
        "    arch = opt.arch\n",
        "    print('You choose '+arch+'...')\n",
        "    if arch == 'UNet':\n",
        "        model_restoration = UNet(dim=opt.embed_dim)\n",
        "    elif arch == 'Uformer':\n",
        "        model_restoration = Uformer(img_size=opt.train_ps,embed_dim=opt.embed_dim,win_size=opt.win_size,token_embed=opt.token_embed,token_mlp=opt.token_mlp)\n",
        "    elif arch == 'Uformer16':\n",
        "        model_restoration = Uformer(img_size=opt.train_ps,embed_dim=16,win_size=8,token_embed='linear',token_mlp='leff')\n",
        "    elif arch == 'Uformer32':\n",
        "        model_restoration = Uformer(img_size=opt.train_ps,embed_dim=32,win_size=8,token_embed='linear',token_mlp='leff')\n",
        "    elif arch == 'Uformer_CatCross':\n",
        "        model_restoration = Uformer_CatCross(img_size=opt.train_ps,embed_dim=opt.embed_dim,win_size=8,token_embed=opt.token_embed,token_mlp=opt.token_mlp)\n",
        "    elif arch == 'Uformer_Cross':\n",
        "        model_restoration = Uformer_Cross(img_size=opt.train_ps,embed_dim=opt.embed_dim,win_size=opt.win_size,token_embed=opt.token_embed,token_mlp=opt.token_mlp)\n",
        "    else:\n",
        "        raise Exception(\"Arch error!\")\n",
        "\n",
        "    return model_restoration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4tNRYt6jImy"
      },
      "source": [
        "### Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqUYUyfQjKFt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tv_loss(x, beta = 0.5, reg_coeff = 5):\n",
        "    '''Calculates TV loss for an image `x`.\n",
        "        \n",
        "    Args:\n",
        "        x: image, torch.Variable of torch.Tensor\n",
        "        beta: See https://arxiv.org/abs/1412.0035 (fig. 2) to see effect of `beta` \n",
        "    '''\n",
        "    dh = torch.pow(x[:,:,:,1:] - x[:,:,:,:-1], 2)\n",
        "    dw = torch.pow(x[:,:,1:,:] - x[:,:,:-1,:], 2)\n",
        "    a,b,c,d=x.shape\n",
        "    return reg_coeff*(torch.sum(torch.pow(dh[:, :, :-1] + dw[:, :, :, :-1], beta))/(a*b*c*d))\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
        "\n",
        "\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        diff = x - y\n",
        "        # loss = torch.sum(torch.sqrt(diff * diff + self.eps))\n",
        "        loss = torch.mean(torch.sqrt((diff * diff) + (self.eps*self.eps)))\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me2fEsVbjVAc"
      },
      "source": [
        "### Warmup Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWoAGpCejXCM"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
        "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
        "\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n",
        "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
        "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        if self.multiplier < 1.:\n",
        "            raise ValueError('multiplier should be greater thant or equal to 1.')\n",
        "        self.total_epoch = total_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        super(GradualWarmupScheduler, self).__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "\n",
        "        if self.multiplier == 1.0:\n",
        "            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
        "        if self.last_epoch <= self.total_epoch:\n",
        "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(metrics, None)\n",
        "            else:\n",
        "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
        "            if self.finished and self.after_scheduler:\n",
        "                if epoch is None:\n",
        "                    self.after_scheduler.step(None)\n",
        "                else:\n",
        "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
        "            else:\n",
        "                return super(GradualWarmupScheduler, self).step(epoch)\n",
        "        else:\n",
        "            self.step_ReduceLROnPlateau(metrics, epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnrHkD50i3zD"
      },
      "source": [
        "# SIDD - Smartphone Image Denoising Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMdyyzHCQlAY"
      },
      "source": [
        "## âš™ï¸ Constantes para el entrenamiento\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKyr6iJ4Uxln"
      },
      "source": [
        "Constantes de configuraciÃ³n del modelo que vamos a generar durante el entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJPZtKQ2oK4r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhbCYTuUX9jW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfSl8y9PUvfP"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a generar\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 32\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# TamaÃ±o del patch para las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# Directorio de entrenamiento: debemos cargarlo de forma manual\n",
        "# y copiar la ruta aquÃ­\n",
        "train_dir = '/content/drive/MyDrive/TFM/SIDD/trainSmall'\n",
        "\n",
        "# Entorno\n",
        "env = 32\n",
        "\n",
        "# Directorio de los datos generados en el entrenamiento\n",
        "val_dir =  '/content/drive/MyDrive/TFM/SIDD/val'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# Warmup\n",
        "warmup = True\n",
        "\n",
        "# Epochs for warmup\n",
        "warmup_epochs = 3\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Optimizador\n",
        "optimizer = 'adamw'\n",
        "\n",
        "# Learning rate inicial\n",
        "lr_initial = 0.0002\n",
        "\n",
        "# Weight decay\n",
        "weight_decay = 0.02\n",
        "\n",
        "# Resume\n",
        "resume = False\n",
        "\n",
        "# Epochs para el entrenamiento\n",
        "nepoch = 100\n",
        "\n",
        "# Train data-loader workers \n",
        "train_workers = 16\n",
        "\n",
        "# Eval data-loader workerks\n",
        "eval_workers = 8\n",
        "\n",
        "# Checkpoint\n",
        "checkpoint = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oalMiG-0p0LA"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8SyudYyp4Rn"
      },
      "outputs": [],
      "source": [
        "class TrainOptions():\n",
        "    def __init__(\n",
        "        self, \n",
        "        arch, \n",
        "        batch_size, \n",
        "        gpu, \n",
        "        train_ps, \n",
        "        train_dir, \n",
        "        env, \n",
        "        val_dir, \n",
        "        embed_dim, \n",
        "        warmout,\n",
        "        warmup_epochs,\n",
        "        win_size,\n",
        "        token_embed,\n",
        "        token_mlp,\n",
        "        optimizer,\n",
        "        lr_initial,\n",
        "        weight_decay,\n",
        "        resume,\n",
        "        nepoch,\n",
        "        train_workers,\n",
        "        eval_workers,\n",
        "        checkpoint):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.train_ps = train_ps\n",
        "        self.train_dir = train_dir\n",
        "        self.env = env\n",
        "        self.val_dir = val_dir\n",
        "        self.embed_dim = embed_dim\n",
        "        self.warmup = warmup\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_initial = lr_initial\n",
        "        self.weight_decay = weight_decay\n",
        "        self.resume = resume\n",
        "        self.nepoch = nepoch\n",
        "        self.train_workers = train_workers\n",
        "        self.eval_workers = eval_workers\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = TrainOptions(\n",
        "    arch, \n",
        "    batch_size, \n",
        "    gpu, \n",
        "    train_ps, \n",
        "    train_dir, \n",
        "    env, \n",
        "    val_dir, \n",
        "    embed_dim, \n",
        "    warmup,\n",
        "    warmup_epochs,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    optimizer,\n",
        "    lr_initial,\n",
        "    weight_decay,\n",
        "    resume,\n",
        "    nepoch,\n",
        "    train_workers,\n",
        "    eval_workers,\n",
        "    checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o7OpPo3j7L6"
      },
      "source": [
        "## ðŸ‹ðŸ»â€â™‚ï¸ ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0B5NHYGmEDg"
      },
      "source": [
        "Importamos y establecemos los parÃ¡metros de la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijj6DeQSTVe8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "######### Set GPUs ###########\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from natsort import natsorted\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "import datetime\n",
        "from pdb import set_trace as stx\n",
        "\n",
        "from tqdm import tqdm \n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from timm.utils import NativeScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7DWUfBomGjO"
      },
      "source": [
        "Establecemos el directorio en el que vamos almacenar los logs de ejecuciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHSjHHmgk4jv"
      },
      "outputs": [],
      "source": [
        "######### Logs dir ###########\n",
        "log_dir = os.path.join('./','log', str(arch)+str(env))\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "logname = os.path.join(log_dir, datetime.datetime.now().isoformat()+'.txt') \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())\n",
        "result_dir = os.path.join(log_dir, 'results')\n",
        "model_dir  = os.path.join(log_dir, 'models')\n",
        "mkdir(result_dir)\n",
        "mkdir(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hex3r0enmxqT"
      },
      "source": [
        "Establecemos las semillas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noZl8MWOmyr8"
      },
      "outputs": [],
      "source": [
        "# ######### Set Seeds ###########\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbRwVzwpm374"
      },
      "source": [
        "Guardamos el modelo en los logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4wKi3cjmy-w"
      },
      "outputs": [],
      "source": [
        "######### Model ###########\n",
        "# En ocasiones se produce un error, hay que volver a ejecutar la secciÃ³n de MODELOS\n",
        "model_restoration = get_arch(opt)\n",
        "\n",
        "with open(logname,'a') as f:\n",
        "    f.write(str(opt)+'\\n')\n",
        "    f.write(str(model_restoration)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3apqVsr038Do"
      },
      "source": [
        "Optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pysRouq8wqGK"
      },
      "outputs": [],
      "source": [
        "######### Optimizer ###########\n",
        "start_epoch = 1\n",
        "if opt.optimizer.lower() == 'adam':\n",
        "    optimizer = optim.Adam(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "elif opt.optimizer.lower() == 'adamw':\n",
        "        optimizer = optim.AdamW(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "else:\n",
        "    raise Exception(\"Error optimizer...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSetxar84hdG"
      },
      "source": [
        "ParalelizaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNu_FlVq39Yi"
      },
      "outputs": [],
      "source": [
        "######### DataParallel ###########\n",
        "model_restoration = torch.nn.DataParallel (model_restoration)\n",
        "model_restoration.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnhftdj9Do6o"
      },
      "source": [
        "Cuando se utiliza RESUME en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfTJ46yn4jju"
      },
      "outputs": [],
      "source": [
        "######### Resume ###########\n",
        "if opt.resume:\n",
        "    path_chk_rest = opt.pretrain_weights\n",
        "    load_checkpoint(model_restoration,path_chk_rest)\n",
        "    start_epoch = load_start_epoch(path_chk_rest) + 1\n",
        "    lr = load_optim(optimizer, path_chk_rest)\n",
        "\n",
        "    for p in optimizer.param_groups: p['lr'] = lr\n",
        "    warmup = False\n",
        "    new_lr = lr\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    print(\"==> Resuming Training with learning rate:\",new_lr)\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-start_epoch+1, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdZxNF1QEFpK"
      },
      "source": [
        "Cuando se utiliza WARMUP en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD2YxiuIDsS0"
      },
      "outputs": [],
      "source": [
        "######### Scheduler ###########\n",
        "if opt.warmup:\n",
        "    print(\"Using warmup and cosine strategy!\")\n",
        "    warmup_epochs = opt.warmup_epochs\n",
        "    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-warmup_epochs, eta_min=1e-6)\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "    scheduler.step()\n",
        "else:\n",
        "    step = 50\n",
        "    print(\"Using StepLR,step={}!\".format(step))\n",
        "    scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2hS28SuFh0F"
      },
      "source": [
        " Criterio de pÃ©rdida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF-4i-WeEKCw"
      },
      "outputs": [],
      "source": [
        "######### Loss ###########\n",
        "criterion = CharbonnierLoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSTTOFGGFoJw"
      },
      "source": [
        "Carga de datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EepCXidcFlJ8"
      },
      "outputs": [],
      "source": [
        "######### DataLoader ###########\n",
        "print('===> Loading datasets')\n",
        "img_options_train = {'patch_size':opt.train_ps}\n",
        "train_dataset = get_training_data(opt.train_dir, img_options_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True, \n",
        "        num_workers=opt.train_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "val_dataset = get_validation_data(opt.val_dir)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=opt.batch_size, shuffle=False, \n",
        "        num_workers=opt.eval_workers, pin_memory=False, drop_last=False)\n",
        "\n",
        "len_trainset = train_dataset.__len__()\n",
        "len_valset = val_dataset.__len__()\n",
        "print(\"Sizeof training set: \", len_trainset,\", sizeof validation set: \", len_valset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad5PCjZNmJkB"
      },
      "source": [
        "ValidaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvk__lR0FpWh"
      },
      "outputs": [],
      "source": [
        "######### validation ###########\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    for ii, data_val in enumerate((val_loader), 0):\n",
        "        target = data_val[0].cuda()\n",
        "        input_ = data_val[1].cuda()\n",
        "        filenames = data_val[2]\n",
        "        psnr_val_rgb.append(batch_PSNR(input_, target, False).item())\n",
        "    psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "    print('Input & GT (PSNR) -->%.4f dB'%(psnr_val_rgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjC0T2ORrpH4"
      },
      "source": [
        "Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxwn9G1ZmLy_"
      },
      "outputs": [],
      "source": [
        "######### train ###########\n",
        "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.nepoch))\n",
        "best_psnr = 0\n",
        "best_epoch = 0\n",
        "best_iter = 0\n",
        "eval_now = len(train_loader)//4\n",
        "print(\"\\nEvaluation after every {} Iterations !!!\\n\".format(eval_now))\n",
        "\n",
        "loss_scaler = NativeScaler()\n",
        "for epoch in range(start_epoch, opt.nepoch + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    train_id = 1\n",
        "    for i, data in enumerate(train_loader, 0): \n",
        "        print(\"Iteration {} of epoch {}\".format(i, epoch))\n",
        "        # zero_grad\n",
        "        #optimizer.zero_grad()\n",
        "\n",
        "        target = data[0].cuda()\n",
        "        input_ = data[1].cuda()\n",
        "\n",
        "        if epoch>5:\n",
        "            target, input_ = MixUp_AUG().aug(target, input_)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            restored = model_restoration(input_)\n",
        "            restored = torch.clamp(restored,0,1)  \n",
        "            loss = criterion(restored, target)\n",
        "        loss_scaler(\n",
        "                loss, optimizer,parameters=model_restoration.parameters())\n",
        "        epoch_loss +=loss.item()\n",
        "\n",
        "        #### Evaluation ####\n",
        "        if (i+1)%eval_now==0 and i>0:\n",
        "            with torch.no_grad():\n",
        "                model_restoration.eval()\n",
        "                psnr_val_rgb = []\n",
        "                for ii, data_val in enumerate((val_loader), 0):\n",
        "                    target = data_val[0].cuda()\n",
        "                    input_ = data_val[1].cuda()\n",
        "                    filenames = data_val[2]\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        restored = model_restoration(input_)\n",
        "                    restored = torch.clamp(restored,0,1)  \n",
        "                    psnr_val_rgb.append(batch_PSNR(restored, target, False).item())\n",
        "\n",
        "                psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "                \n",
        "                if psnr_val_rgb > best_psnr:\n",
        "                    best_psnr = psnr_val_rgb\n",
        "                    best_epoch = epoch\n",
        "                    best_iter = i \n",
        "                    torch.save({'epoch': epoch, \n",
        "                                'state_dict': model_restoration.state_dict(),\n",
        "                                'optimizer' : optimizer.state_dict()\n",
        "                                }, os.path.join(model_dir,\"model_best.pth\"))\n",
        "\n",
        "                print(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr))\n",
        "                with open(logname,'a') as f:\n",
        "                    f.write(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" \\\n",
        "                        % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr)+'\\n')\n",
        "                model_restoration.train()\n",
        "                torch.cuda.empty_cache()\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0]))\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    with open(logname,'a') as f:\n",
        "        f.write(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0])+'\\n')\n",
        "\n",
        "    torch.save({'epoch': epoch, \n",
        "                'state_dict': model_restoration.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict()\n",
        "                }, os.path.join(model_dir,\"model_latest.pth\"))   \n",
        "\n",
        "    if epoch%opt.checkpoint == 0:\n",
        "        torch.save({'epoch': epoch, \n",
        "                    'state_dict': model_restoration.state_dict(),\n",
        "                    'optimizer' : optimizer.state_dict()\n",
        "                    }, os.path.join(model_dir,\"model_epoch_{}.pth\".format(epoch))) \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnfblf5GADb"
      },
      "source": [
        "## âš™ï¸ Constantes para la evaluaciÃ³n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNPOh3qsGS8z"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo generado durante el entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfoyIcqGrwGk"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/SIDD/val'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/SIDD/results'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/models/uformer.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRlABVFjLxXp"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwl5Tfx0Jhv-"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pHH0KZiRCJL"
      },
      "source": [
        "## ðŸ§ª EVALUACIÃ“N\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-1xZyeYRPjl"
      },
      "source": [
        "Importamos las librerÃ­as y establecemos los parÃ¡metros de las GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Up2lFNqPt2F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os,sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "import scipy.io as sio\n",
        "\n",
        "from skimage import img_as_float32, img_as_ubyte\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr_loss\n",
        "from skimage.metrics import structural_similarity as ssim_loss\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcoRaX3ySv2J"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYfyovR3RR5o"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNTQIJbLS3-I"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV6M3wBGS0nS"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22XlGbNxTCtH"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cpv1I1OS53j"
      },
      "outputs": [],
      "source": [
        "# Si da fallo este mÃ©todo se debe ejecutar la secciÃ³n de MODELOS de nuevo\n",
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drRxMxzhUPun"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYnfvGw4TElu"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJX3yq-UbxS"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyFSrwulUSGO"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUSCLUg4UeQK"
      },
      "outputs": [],
      "source": [
        "print(\"number of GFLOPs: %.2f G\"%(model_restoration.flops() / 1e9))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_PNaAJgLodJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqsXp-IqjHk8"
      },
      "source": [
        "# Convallaria Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gl1i1nuEuUkq"
      },
      "source": [
        "## â™»ï¸ PreparaciÃ³n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH5jfPAHuvIU"
      },
      "source": [
        "Dividimos las imÃ¡genes de tal forma que tengan unas dimensiones de 256 x 256.\n",
        "\n",
        "Las imÃ¡genes del dataset **Convallaria** son de 512 x 512 por lo que tenemos que dividirlas en cuatro partes de 256 x 256 cada una para todos los directorios tanto de entrenamiento como de evaluaciÃ³n.\n",
        "\n",
        "Definimos la funciÃ³n necesaria para la divisiÃ³n de las imÃ¡genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-37obpQFDZQ2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WQ8n9y3xzmY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def divide_image_convallaria(dir_origen, archivo, dir_destino):\n",
        "\n",
        "  # Creamos la carpeta destino\n",
        "  mkdir(dir_destino)\n",
        "\n",
        "  # Cargamos la imagen\n",
        "  img = cv2.imread(dir_origen + '/' + archivo)\n",
        "  \n",
        "  # DIVISIÃ“N VERTICAL\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  width_cutoff = width // 2\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  left1 = img[:, :width_cutoff]\n",
        "  right1 = img[:, width_cutoff:]\n",
        "\n",
        "  # DIVISIÃ“N HORIZONTAL DE LA IZQUIERDA\n",
        "  # Rotamos la imagen\n",
        "  img = cv2.rotate(left1, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  width_cutoff = width // 2\n",
        "  l1 = img[:, :width_cutoff]\n",
        "  l2 = img[:, width_cutoff:]\n",
        "\n",
        "  # Rotamos las dos partes\n",
        "  l1 = cv2.rotate(l1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "  l2 = cv2.rotate(l2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "  # Guardamos las imagenes\n",
        "  print('Guardando...')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_1.png')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_2.png')\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_1.png', l2)\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_2.png', l1)\n",
        "\n",
        "  # DIVISIÃ“N HORIZONTAL DE LA DERECHA\n",
        "  # Rotamos la imagen\n",
        "  img = cv2.rotate(right1, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  width_cutoff = width // 2\n",
        "  r1 = img[:, :width_cutoff]\n",
        "  r2 = img[:, width_cutoff:]\n",
        "\n",
        "  # Rotamos las dos partes\n",
        "  r1 = cv2.rotate(r1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "  r2 = cv2.rotate(r2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "  # Guardamos las imagenes\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_3.png')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_4.png')\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_3.png', r2)\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_4.png', r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xOZM14zyM1u"
      },
      "source": [
        "FunciÃ³n que va a iterar en todos los elementos del directorio seleccionado aplicando la funciÃ³n de recorte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DURv9seHxwZI"
      },
      "outputs": [],
      "source": [
        "def iterate_and_cut(dir_name, dir_destino):\n",
        "  for filename in os.listdir(dir_name):\n",
        "    divide_image_convallaria(dir_name, filename, dir_destino)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V4_bgVj1llV"
      },
      "source": [
        "Cortamos las fotos de cada directorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgqYRBdJ1lJp"
      },
      "outputs": [],
      "source": [
        "# TRAIN\n",
        "dir_origen_train_gt = '/content/drive/MyDrive/TFM/Convallaria/dataset/train/groundtruth'\n",
        "dir_dest_train_gt = '/content/drive/MyDrive/TFM/Convallaria/dataset/train/cut/groundtruth'\n",
        "iterate_and_cut(dir_origen_train_gt, dir_dest_train_gt)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "dir_origen_train_in = '/content/drive/MyDrive/TFM/Convallaria/dataset/train/input'\n",
        "dir_dest_train_in = '/content/drive/MyDrive/TFM/Convallaria/dataset/train/cut/input'\n",
        "iterate_and_cut(dir_origen_train_in, dir_dest_train_in)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "# VAL\n",
        "dir_origen_val_gt = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/groundtruth'\n",
        "dir_dest_val_gt = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/groundtruth'\n",
        "iterate_and_cut(dir_origen_val_gt, dir_dest_val_gt)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "dir_origen_val_in = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/input'\n",
        "dir_dest_val_in = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/input'\n",
        "iterate_and_cut(dir_origen_val_in, dir_dest_val_in)\n",
        "print('\\n##############################\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxRP9mlGjy7F"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N PRE-TRAINED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDjrNy7looRM"
      },
      "source": [
        "Importamos las librerÃ­as y establecemos los parÃ¡metros de las GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OQovWYkooRM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os,sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "sys.path.append('/home/ma-user/work/uformer_for_denoise')\n",
        "\n",
        "import scipy.io as sio\n",
        "\n",
        "from skimage import img_as_float32, img_as_ubyte\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr_loss\n",
        "from skimage.metrics import structural_similarity as ssim_loss\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfP79lcjlHgI"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHX2jl9jjypb"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/Convallaria/results-pretrained'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/Convallaria/uformer32_denoising_sidd.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFvXzPvslk0N"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYAp20d7lk0Y"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvrtBPLzlakZ"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BddnkWb_lakl"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2ikQD4-lakl"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAe4yTH4lakl"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "064fL5KPlakl"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnQJCmArlakm"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuqHvjt7lakm"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqtGFubelakm"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9V_GbT9lakm"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y196Z-g_lakm"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQvpgbyenyN0"
      },
      "source": [
        "## ðŸ‹ðŸ»â€â™‚ï¸ ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNw96-3AoGCx"
      },
      "source": [
        "Constantes de configuraciÃ³n del modelo que vamos a generar durante el entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao3jmME6oGCy"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a generar\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 16\n",
        "\n",
        "# GPUs\n",
        "gpu = '1,2'\n",
        "\n",
        "# TamaÃ±o del patch para las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# Directorio de entrenamiento: debemos cargarlo de forma manual\n",
        "# y copiar la ruta aquÃ­\n",
        "train_dir = '/content/drive/MyDrive/TFM/Convallaria/dataset/train/cut'\n",
        "\n",
        "# Entorno\n",
        "env = 32\n",
        "\n",
        "# Directorio de los datos generados en el entrenamiento\n",
        "val_dir =  '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# Warmup\n",
        "warmup = True\n",
        "\n",
        "# Epochs for warmup\n",
        "warmup_epochs = 3\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Optimizador\n",
        "optimizer = 'adamw'\n",
        "\n",
        "# Learning rate inicial\n",
        "lr_initial = 0.0002\n",
        "\n",
        "# Weight decay\n",
        "weight_decay = 0.02\n",
        "\n",
        "# Resume\n",
        "resume = False\n",
        "\n",
        "# Epochs para el entrenamiento\n",
        "nepoch = 100\n",
        "\n",
        "# Train data-loader workers \n",
        "train_workers = 16\n",
        "\n",
        "# Eval data-loader workerks\n",
        "eval_workers = 8\n",
        "\n",
        "# Checkpoint\n",
        "checkpoint = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-QjVjlkoGCy"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUq8UyPvoGCy"
      },
      "outputs": [],
      "source": [
        "class TrainOptions():\n",
        "    def __init__(\n",
        "        self, \n",
        "        arch, \n",
        "        batch_size, \n",
        "        gpu, \n",
        "        train_ps, \n",
        "        train_dir, \n",
        "        env, \n",
        "        val_dir, \n",
        "        embed_dim, \n",
        "        warmout,\n",
        "        warmup_epochs,\n",
        "        win_size,\n",
        "        token_embed,\n",
        "        token_mlp,\n",
        "        optimizer,\n",
        "        lr_initial,\n",
        "        weight_decay,\n",
        "        resume,\n",
        "        nepoch,\n",
        "        train_workers,\n",
        "        eval_workers,\n",
        "        checkpoint):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.train_ps = train_ps\n",
        "        self.train_dir = train_dir\n",
        "        self.env = env\n",
        "        self.val_dir = val_dir\n",
        "        self.embed_dim = embed_dim\n",
        "        self.warmup = warmup\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_initial = lr_initial\n",
        "        self.weight_decay = weight_decay\n",
        "        self.resume = resume\n",
        "        self.nepoch = nepoch\n",
        "        self.train_workers = train_workers\n",
        "        self.eval_workers = eval_workers\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = TrainOptions(\n",
        "    arch, \n",
        "    batch_size, \n",
        "    gpu, \n",
        "    train_ps, \n",
        "    train_dir, \n",
        "    env, \n",
        "    val_dir, \n",
        "    embed_dim, \n",
        "    warmup,\n",
        "    warmup_epochs,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    optimizer,\n",
        "    lr_initial,\n",
        "    weight_decay,\n",
        "    resume,\n",
        "    nepoch,\n",
        "    train_workers,\n",
        "    eval_workers,\n",
        "    checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEBar03xoGCy"
      },
      "source": [
        "Importamos y establecemos los parÃ¡metros de la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DryM33QEoGCz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "######### Set GPUs ###########\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from natsort import natsorted\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "import datetime\n",
        "from pdb import set_trace as stx\n",
        "\n",
        "from tqdm import tqdm \n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from timm.utils import NativeScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUppH7mloGCz"
      },
      "source": [
        "Establecemos el directorio en el que vamos almacenar los logs de ejecuciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NulORc_loGCz"
      },
      "outputs": [],
      "source": [
        "######### Logs dir ###########\n",
        "log_dir = os.path.join('/content/drive/MyDrive/TFM/Convallaria/log', str(arch)+str(env))\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "logname = os.path.join(log_dir, datetime.datetime.now().isoformat()+'.txt') \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())\n",
        "result_dir = os.path.join(log_dir, 'results')\n",
        "model_dir  = os.path.join(log_dir, 'models')\n",
        "mkdir(result_dir)\n",
        "mkdir(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CeGCQq0oGCz"
      },
      "source": [
        "Establecemos las semillas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXaUhyvoGCz"
      },
      "outputs": [],
      "source": [
        "# ######### Set Seeds ###########\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNYu8v1oGCz"
      },
      "source": [
        "Guardamos el modelo en los logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvHyw0u-oGCz"
      },
      "outputs": [],
      "source": [
        "######### Model ###########\n",
        "# En ocasiones se produce un error, hay que volver a ejecutar la secciÃ³n de MODELOS\n",
        "model_restoration = get_arch(opt)\n",
        "\n",
        "with open(logname,'a') as f:\n",
        "    f.write(str(opt)+'\\n')\n",
        "    f.write(str(model_restoration)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwGB0fBdoGCz"
      },
      "source": [
        "Optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUilGVVkoGCz"
      },
      "outputs": [],
      "source": [
        "######### Optimizer ###########\n",
        "start_epoch = 1\n",
        "if opt.optimizer.lower() == 'adam':\n",
        "    optimizer = optim.Adam(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "elif opt.optimizer.lower() == 'adamw':\n",
        "        optimizer = optim.AdamW(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "else:\n",
        "    raise Exception(\"Error optimizer...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghiZUt5aoGCz"
      },
      "source": [
        "ParalelizaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP48O_MPoGC0"
      },
      "outputs": [],
      "source": [
        "######### DataParallel ###########\n",
        "model_restoration = torch.nn.DataParallel (model_restoration)\n",
        "model_restoration.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCysQqHsoGC0"
      },
      "source": [
        "Cuando se utiliza RESUME en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_jVFskNoGC0"
      },
      "outputs": [],
      "source": [
        "######### Resume ###########\n",
        "if opt.resume:\n",
        "    path_chk_rest = opt.pretrain_weights\n",
        "    load_checkpoint(model_restoration,path_chk_rest)\n",
        "    start_epoch = load_start_epoch(path_chk_rest) + 1\n",
        "    lr = load_optim(optimizer, path_chk_rest)\n",
        "\n",
        "    for p in optimizer.param_groups: p['lr'] = lr\n",
        "    warmup = False\n",
        "    new_lr = lr\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    print(\"==> Resuming Training with learning rate:\",new_lr)\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-start_epoch+1, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqh0PtVoGC0"
      },
      "source": [
        "Cuando se utiliza WARMUP en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXW0NU-soGC0"
      },
      "outputs": [],
      "source": [
        "######### Scheduler ###########\n",
        "if opt.warmup:\n",
        "    print(\"Using warmup and cosine strategy!\")\n",
        "    warmup_epochs = opt.warmup_epochs\n",
        "    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-warmup_epochs, eta_min=1e-6)\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "    scheduler.step()\n",
        "else:\n",
        "    step = 50\n",
        "    print(\"Using StepLR,step={}!\".format(step))\n",
        "    scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvcSCZ36oGC0"
      },
      "source": [
        " Criterio de pÃ©rdida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AXpro5WoGC0"
      },
      "outputs": [],
      "source": [
        "######### Loss ###########\n",
        "criterion = CharbonnierLoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcpD16z3oGC0"
      },
      "source": [
        "Carga de datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nti7FqInoGC0"
      },
      "outputs": [],
      "source": [
        "######### DataLoader ###########\n",
        "print('===> Loading datasets')\n",
        "img_options_train = {'patch_size':opt.train_ps}\n",
        "train_dataset = get_training_data(opt.train_dir, img_options_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True, \n",
        "        num_workers=opt.train_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "val_dataset = get_validation_data(opt.val_dir)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=opt.batch_size, shuffle=False, \n",
        "        num_workers=opt.eval_workers, pin_memory=False, drop_last=False)\n",
        "\n",
        "len_trainset = train_dataset.__len__()\n",
        "len_valset = val_dataset.__len__()\n",
        "print(\"Sizeof training set: \", len_trainset,\", sizeof validation set: \", len_valset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J1V8NJKoGC1"
      },
      "source": [
        "ValidaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XsGVXPEoGC1"
      },
      "outputs": [],
      "source": [
        "######### validation ###########\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    for ii, data_val in enumerate((val_loader), 0):\n",
        "        target = data_val[0].cuda()\n",
        "        input_ = data_val[1].cuda()\n",
        "        filenames = data_val[2]\n",
        "        psnr_val_rgb.append(batch_PSNR(input_, target, False).item())\n",
        "    psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "    print('Input & GT (PSNR) -->%.4f dB'%(psnr_val_rgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQL_30vAoGC1"
      },
      "source": [
        "Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDy8yp_ATWG2"
      },
      "outputs": [],
      "source": [
        "######### train ###########\n",
        "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.nepoch))\n",
        "best_psnr = 0\n",
        "best_epoch = 0\n",
        "best_iter = 0\n",
        "eval_now = len(train_loader)//4\n",
        "print(\"\\nEvaluation after every {} Iterations !!!\\n\".format(eval_now))\n",
        "\n",
        "loss_scaler = NativeScaler()\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(start_epoch, opt.nepoch + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    train_id = 1\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0): \n",
        "        print(\"Iteration {} of epoch {}\".format(i, epoch))\n",
        "        # zero_grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = data[0].cuda()\n",
        "        input_ = data[1].cuda()\n",
        "\n",
        "        if epoch>5:\n",
        "            target, input_ = MixUp_AUG().aug(target, input_)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            restored = model_restoration(input_)\n",
        "            restored = torch.clamp(restored,0,1)  \n",
        "            loss = criterion(restored, target)\n",
        "        loss_scaler(\n",
        "                loss, optimizer,parameters=model_restoration.parameters())\n",
        "        epoch_loss +=loss.item()\n",
        "\n",
        "        #### Evaluation ####\n",
        "        if (i+1)%eval_now==0 and i>0:\n",
        "            with torch.no_grad():\n",
        "                model_restoration.eval()\n",
        "                psnr_val_rgb = []\n",
        "                for ii, data_val in enumerate((val_loader), 0):\n",
        "                    target = data_val[0].cuda()\n",
        "                    input_ = data_val[1].cuda()\n",
        "                    filenames = data_val[2]\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        restored = model_restoration(input_)\n",
        "                    restored = torch.clamp(restored,0,1)  \n",
        "                    psnr_val_rgb.append(batch_PSNR(restored, target, False).item())\n",
        "\n",
        "                psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "                \n",
        "                if psnr_val_rgb > best_psnr:\n",
        "                    best_psnr = psnr_val_rgb\n",
        "                    best_epoch = epoch\n",
        "                    best_iter = i \n",
        "                    torch.save({'epoch': epoch, \n",
        "                                'state_dict': model_restoration.state_dict(),\n",
        "                                'optimizer' : optimizer.state_dict()\n",
        "                                }, os.path.join(model_dir,\"model_best.pth\"))\n",
        "\n",
        "                print(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr))\n",
        "                with open(logname,'a') as f:\n",
        "                    f.write(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" \\\n",
        "                        % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr)+'\\n')\n",
        "                model_restoration.train()\n",
        "                torch.cuda.empty_cache()\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0]))\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    with open(logname,'a') as f:\n",
        "        f.write(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0])+'\\n')\n",
        "\n",
        "    torch.save({'epoch': epoch, \n",
        "                'state_dict': model_restoration.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict()\n",
        "                }, os.path.join(model_dir,\"model_latest.pth\"))   \n",
        "\n",
        "    if epoch%opt.checkpoint == 0:\n",
        "        torch.save({'epoch': epoch, \n",
        "                    'state_dict': model_restoration.state_dict(),\n",
        "                    'optimizer' : optimizer.state_dict()\n",
        "                    }, os.path.join(model_dir,\"model_epoch_{}.pth\".format(epoch))) \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kD1MVq_4SGK"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N DESPUÃ‰S DEL ENTRENAMIENTO (con el mejor modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2NlWRi4SGK"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23DWnmYr4SGK"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/Convallaria/results-postrained/'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/Convallaria/log/Uformer32/models/model_best.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBFLjPOm4SGK"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrvX7-UV4SGK"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd061pLI4SGL"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bOSR_AE4SGL"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlbVu_RD4SGL"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOGmhJb4SGL"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxhHctQz4SGL"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVxrNTp94SGL"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSTEnxjN4SGL"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWZIKCMa4SGM"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKcC7Z484SGM"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hUp92nF4SGM"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HzFvbpR56VJ"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N DESPUÃ‰S DEL ENTRENAMIENTO (con el Ãºltimo modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybAbVZMH56VK"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG_SuiV556VK"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/Convallaria/results-postrained-latest'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/Convallaria/log/Uformer32/models/model_latest.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfvvCI0a56VK"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzGJbXUH56VK"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGv4ga8D56VK"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvm04xKB56VK"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-3iD9c56VK"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sucFN9056VK"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O564MO56VK"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTmUX1Ez56VL"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNsA_PIB56VL"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY6vFLuv56VL"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NecrbMT56VL"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7IkuDMT56VL"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ufLvMKyXG-"
      },
      "source": [
        "##Imagenes resultantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OCPt5Jp0A68"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDz6hQFAy1hs"
      },
      "outputs": [],
      "source": [
        "imgInput = plt.imread('/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/input/90_1.png')\n",
        "\n",
        "imgGT = plt.imread('/content/drive/MyDrive/TFM/Convallaria/dataset/val/cut/groundtruth/90_1.png')\n",
        "\n",
        "imgResult = plt.imread('/content/drive/MyDrive/TFM/Convallaria/results-postrained/90_1.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM3fRefKyWsU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(label='Input Image')\n",
        "plt.imshow(imgInput[100:200,150:250])\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(label='Groundtruth')\n",
        "plt.imshow(imgGT[100:200,150:250])\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(label='Result')\n",
        "plt.imshow(imgResult[100:200,150:250])\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ydaJGHlI9U"
      },
      "source": [
        "# Mouse Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgZB1gyolI9V"
      },
      "source": [
        "## â™»ï¸ PreparaciÃ³n de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh142KdElI9V"
      },
      "source": [
        "Dividimos las imÃ¡genes de tal forma que tengan unas dimensiones de 256 x 256.\n",
        "\n",
        "Las imÃ¡genes del dataset **Convallaria** son de 512 x 512 por lo que tenemos que dividirlas en cuatro partes de 256 x 256 cada una para todos los directorios tanto de entrenamiento como de evaluaciÃ³n.\n",
        "\n",
        "Definimos la funciÃ³n necesaria para la divisiÃ³n de las imÃ¡genes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IPgKQywlI9W"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1vjLXHNlI9W"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def divide_image_convallaria(dir_origen, archivo, dir_destino):\n",
        "\n",
        "  # Creamos la carpeta destino\n",
        "  mkdir(dir_destino)\n",
        "\n",
        "  # Cargamos la imagen\n",
        "  img = cv2.imread(dir_origen + '/' + archivo)\n",
        "  \n",
        "  # DIVISIÃ“N VERTICAL\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  width_cutoff = width // 2\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  left1 = img[:, :width_cutoff]\n",
        "  right1 = img[:, width_cutoff:]\n",
        "\n",
        "  # DIVISIÃ“N HORIZONTAL DE LA IZQUIERDA\n",
        "  # Rotamos la imagen\n",
        "  img = cv2.rotate(left1, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  width_cutoff = width // 2\n",
        "  l1 = img[:, :width_cutoff]\n",
        "  l2 = img[:, width_cutoff:]\n",
        "\n",
        "  # Rotamos las dos partes\n",
        "  l1 = cv2.rotate(l1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "  l2 = cv2.rotate(l2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "  # Guardamos las imagenes\n",
        "  print('Guardando...')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_1.png')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_2.png')\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_1.png', l2)\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_2.png', l1)\n",
        "\n",
        "  # DIVISIÃ“N HORIZONTAL DE LA DERECHA\n",
        "  # Rotamos la imagen\n",
        "  img = cv2.rotate(right1, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "  # Cortamos la imagen por la mitad\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "\n",
        "  # Dividimos la imagen en dos partes\n",
        "  width_cutoff = width // 2\n",
        "  r1 = img[:, :width_cutoff]\n",
        "  r2 = img[:, width_cutoff:]\n",
        "\n",
        "  # Rotamos las dos partes\n",
        "  r1 = cv2.rotate(r1, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "  r2 = cv2.rotate(r2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "\n",
        "  # Guardamos las imagenes\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_3.png')\n",
        "  print(dir_destino + '/' + archivo[:-4] + '_4.png')\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_3.png', r2)\n",
        "  cv2.imwrite(dir_destino + '/' + archivo[:-4] + '_4.png', r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zp8hnYOlI9W"
      },
      "source": [
        "FunciÃ³n que va a iterar en todos los elementos del directorio seleccionado aplicando la funciÃ³n de recorte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sh_Qx-4lI9X"
      },
      "outputs": [],
      "source": [
        "def iterate_and_cut(dir_name, dir_destino):\n",
        "  for filename in os.listdir(dir_name):\n",
        "    divide_image_convallaria(dir_name, filename, dir_destino)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q3MeN4VlI9X"
      },
      "source": [
        "Cortamos las fotos de cada directorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6gpwIKlI9X"
      },
      "outputs": [],
      "source": [
        "# TRAIN\n",
        "dir_origen_train_gt = '/content/drive/MyDrive/TFM/data/Confocal_MICE/train/groundtruth'\n",
        "dir_dest_train_gt = '/content/drive/MyDrive/TFM/data/Confocal_MICE/train/cut/groundtruth'\n",
        "iterate_and_cut(dir_origen_train_gt, dir_dest_train_gt)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "dir_origen_train_in = '/content/drive/MyDrive/TFM/data/Confocal_MICE/train/input'\n",
        "dir_dest_train_in = '/content/drive/MyDrive/TFM/data/Confocal_MICE/train/cut/input'\n",
        "iterate_and_cut(dir_origen_train_in, dir_dest_train_in)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "# VAL\n",
        "dir_origen_val_gt = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/groundtruth'\n",
        "dir_dest_val_gt = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/groundtruth'\n",
        "iterate_and_cut(dir_origen_val_gt, dir_dest_val_gt)\n",
        "print('\\n##############################\\n')\n",
        "\n",
        "dir_origen_val_in = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/input'\n",
        "dir_dest_val_in = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/input'\n",
        "iterate_and_cut(dir_origen_val_in, dir_dest_val_in)\n",
        "print('\\n##############################\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmEvoDvQlI9X"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N PRE-TRAINED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30149wCdlI9Y"
      },
      "source": [
        "Importamos las librerÃ­as y establecemos los parÃ¡metros de las GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9SMpjVFlI9Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os,sys\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "sys.path.append('/home/ma-user/work/uformer_for_denoise')\n",
        "\n",
        "import scipy.io as sio\n",
        "\n",
        "from skimage import img_as_float32, img_as_ubyte\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr_loss\n",
        "from skimage.metrics import structural_similarity as ssim_loss\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = opt.gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJVjikjlI9Y"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIy__RhdlI9Y"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/results-pretrained'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/data/Confocal_MICE/uformer32_denoising_sidd.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOEXle-zlI9Y"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SSVUW_klI9Z"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FbOZnK8lI9Z"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVnLoD3plI9Z"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71wXjfvelI9Z"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nZfkpdXlI9Z"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9KDUzFmlI9Z"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYdUjVkflI9a"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33AZIKglI9a"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NOwrY1_lI9a"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtX1-IsNlI9a"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os4i4pZPlI9a"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi4lirFplI9a"
      },
      "source": [
        "## ðŸ‹ðŸ»â€â™‚ï¸ ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p868v6UglI9b"
      },
      "source": [
        "Constantes de configuraciÃ³n del modelo que vamos a generar durante el entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DunVoyllI9b"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a generar\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 16\n",
        "\n",
        "# GPUs\n",
        "gpu = '1,2'\n",
        "\n",
        "# TamaÃ±o del patch para las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# Directorio de entrenamiento: debemos cargarlo de forma manual\n",
        "# y copiar la ruta aquÃ­\n",
        "train_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/train/cut'\n",
        "\n",
        "# Entorno\n",
        "env = 32\n",
        "\n",
        "# Directorio de los datos generados en el entrenamiento\n",
        "val_dir =  '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# Warmup\n",
        "warmup = True\n",
        "\n",
        "# Epochs for warmup\n",
        "warmup_epochs = 3\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Optimizador\n",
        "optimizer = 'adamw'\n",
        "\n",
        "# Learning rate inicial\n",
        "lr_initial = 0.0002\n",
        "\n",
        "# Weight decay\n",
        "weight_decay = 0.02\n",
        "\n",
        "# Resume\n",
        "resume = False\n",
        "\n",
        "# Epochs para el entrenamiento\n",
        "nepoch = 100\n",
        "\n",
        "# Train data-loader workers \n",
        "train_workers = 16\n",
        "\n",
        "# Eval data-loader workerks\n",
        "eval_workers = 8\n",
        "\n",
        "# Checkpoint\n",
        "checkpoint = 50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z8fvAsSlI9b"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Rq7moelI9b"
      },
      "outputs": [],
      "source": [
        "class TrainOptions():\n",
        "    def __init__(\n",
        "        self, \n",
        "        arch, \n",
        "        batch_size, \n",
        "        gpu, \n",
        "        train_ps, \n",
        "        train_dir, \n",
        "        env, \n",
        "        val_dir, \n",
        "        embed_dim, \n",
        "        warmout,\n",
        "        warmup_epochs,\n",
        "        win_size,\n",
        "        token_embed,\n",
        "        token_mlp,\n",
        "        optimizer,\n",
        "        lr_initial,\n",
        "        weight_decay,\n",
        "        resume,\n",
        "        nepoch,\n",
        "        train_workers,\n",
        "        eval_workers,\n",
        "        checkpoint):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.train_ps = train_ps\n",
        "        self.train_dir = train_dir\n",
        "        self.env = env\n",
        "        self.val_dir = val_dir\n",
        "        self.embed_dim = embed_dim\n",
        "        self.warmup = warmup\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.optimizer = optimizer\n",
        "        self.lr_initial = lr_initial\n",
        "        self.weight_decay = weight_decay\n",
        "        self.resume = resume\n",
        "        self.nepoch = nepoch\n",
        "        self.train_workers = train_workers\n",
        "        self.eval_workers = eval_workers\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = TrainOptions(\n",
        "    arch, \n",
        "    batch_size, \n",
        "    gpu, \n",
        "    train_ps, \n",
        "    train_dir, \n",
        "    env, \n",
        "    val_dir, \n",
        "    embed_dim, \n",
        "    warmup,\n",
        "    warmup_epochs,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    optimizer,\n",
        "    lr_initial,\n",
        "    weight_decay,\n",
        "    resume,\n",
        "    nepoch,\n",
        "    train_workers,\n",
        "    eval_workers,\n",
        "    checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5-2z7BslI9b"
      },
      "source": [
        "Importamos y establecemos los parÃ¡metros de la GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kHNm2WPlI9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "######### Set GPUs ###########\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# print(device)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from natsort import natsorted\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "import datetime\n",
        "from pdb import set_trace as stx\n",
        "\n",
        "from tqdm import tqdm \n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from timm.utils import NativeScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KrBgdfdlI9d"
      },
      "source": [
        "Establecemos el directorio en el que vamos almacenar los logs de ejecuciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuypiQj6lI9d"
      },
      "outputs": [],
      "source": [
        "######### Logs dir ###########\n",
        "log_dir = os.path.join('/content/drive/MyDrive/TFM/data/Confocal_MICE/log', str(arch)+str(env))\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "logname = os.path.join(log_dir, datetime.datetime.now().isoformat()+'.txt') \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())\n",
        "result_dir = os.path.join(log_dir, 'results')\n",
        "model_dir  = os.path.join(log_dir, 'models')\n",
        "mkdir(result_dir)\n",
        "mkdir(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVA070DdlI9d"
      },
      "source": [
        "Establecemos las semillas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCmvpHDhlI9d"
      },
      "outputs": [],
      "source": [
        "# ######### Set Seeds ###########\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "torch.cuda.manual_seed_all(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43VGB9telI9d"
      },
      "source": [
        "Guardamos el modelo en los logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVAnUkxjlI9d"
      },
      "outputs": [],
      "source": [
        "######### Model ###########\n",
        "# En ocasiones se produce un error, hay que volver a ejecutar la secciÃ³n de MODELOS\n",
        "model_restoration = get_arch(opt)\n",
        "\n",
        "with open(logname,'a') as f:\n",
        "    f.write(str(opt)+'\\n')\n",
        "    f.write(str(model_restoration)+'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSUWqhzplI9d"
      },
      "source": [
        "Optimizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvl5ImN8lI9d"
      },
      "outputs": [],
      "source": [
        "######### Optimizer ###########\n",
        "start_epoch = 1\n",
        "if opt.optimizer.lower() == 'adam':\n",
        "    optimizer = optim.Adam(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "elif opt.optimizer.lower() == 'adamw':\n",
        "        optimizer = optim.AdamW(model_restoration.parameters(), lr=opt.lr_initial, betas=(0.9, 0.999),eps=1e-8, weight_decay=opt.weight_decay)\n",
        "else:\n",
        "    raise Exception(\"Error optimizer...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orAuP6O1lI9d"
      },
      "source": [
        "ParalelizaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeEgfCwMlI9d"
      },
      "outputs": [],
      "source": [
        "######### DataParallel ###########\n",
        "model_restoration = torch.nn.DataParallel (model_restoration)\n",
        "model_restoration.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVmc9YJ6lI9d"
      },
      "source": [
        "Cuando se utiliza RESUME en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUz4U_9klI9e"
      },
      "outputs": [],
      "source": [
        "######### Resume ###########\n",
        "if opt.resume:\n",
        "    path_chk_rest = opt.pretrain_weights\n",
        "    load_checkpoint(model_restoration,path_chk_rest)\n",
        "    start_epoch = load_start_epoch(path_chk_rest) + 1\n",
        "    lr = load_optim(optimizer, path_chk_rest)\n",
        "\n",
        "    for p in optimizer.param_groups: p['lr'] = lr\n",
        "    warmup = False\n",
        "    new_lr = lr\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    print(\"==> Resuming Training with learning rate:\",new_lr)\n",
        "    print('------------------------------------------------------------------------------')\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-start_epoch+1, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfZr1jS8lI9e"
      },
      "source": [
        "Cuando se utiliza WARMUP en las opciones de configuraciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUZGMQDFlI9e"
      },
      "outputs": [],
      "source": [
        "######### Scheduler ###########\n",
        "if opt.warmup:\n",
        "    print(\"Using warmup and cosine strategy!\")\n",
        "    warmup_epochs = opt.warmup_epochs\n",
        "    scheduler_cosine = optim.lr_scheduler.CosineAnnealingLR(optimizer, opt.nepoch-warmup_epochs, eta_min=1e-6)\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=scheduler_cosine)\n",
        "    scheduler.step()\n",
        "else:\n",
        "    step = 50\n",
        "    print(\"Using StepLR,step={}!\".format(step))\n",
        "    scheduler = StepLR(optimizer, step_size=step, gamma=0.5)\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4mvqTF1lI9e"
      },
      "source": [
        " Criterio de pÃ©rdida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7O1BsRjlI9e"
      },
      "outputs": [],
      "source": [
        "######### Loss ###########\n",
        "criterion = CharbonnierLoss().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgTAunWHlI9e"
      },
      "source": [
        "Carga de datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbaNuAUElI9e"
      },
      "outputs": [],
      "source": [
        "######### DataLoader ###########\n",
        "print('===> Loading datasets')\n",
        "img_options_train = {'patch_size':opt.train_ps}\n",
        "train_dataset = get_training_data(opt.train_dir, img_options_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=opt.batch_size, shuffle=True, \n",
        "        num_workers=opt.train_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "val_dataset = get_validation_data(opt.val_dir)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=opt.batch_size, shuffle=False, \n",
        "        num_workers=opt.eval_workers, pin_memory=False, drop_last=False)\n",
        "\n",
        "len_trainset = train_dataset.__len__()\n",
        "len_valset = val_dataset.__len__()\n",
        "print(\"Sizeof training set: \", len_trainset,\", sizeof validation set: \", len_valset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFtiuMTSlI9e"
      },
      "source": [
        "ValidaciÃ³n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hrdacswlI9e"
      },
      "outputs": [],
      "source": [
        "######### validation ###########\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    for ii, data_val in enumerate((val_loader), 0):\n",
        "        target = data_val[0].cuda()\n",
        "        input_ = data_val[1].cuda()\n",
        "        filenames = data_val[2]\n",
        "        psnr_val_rgb.append(batch_PSNR(input_, target, False).item())\n",
        "    psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "    print('Input & GT (PSNR) -->%.4f dB'%(psnr_val_rgb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73cYoHOFlI9e"
      },
      "source": [
        "Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb_H2UfzlI9e"
      },
      "outputs": [],
      "source": [
        "######### train ###########\n",
        "print('===> Start Epoch {} End Epoch {}'.format(start_epoch,opt.nepoch))\n",
        "best_psnr = 0\n",
        "best_epoch = 0\n",
        "best_iter = 0\n",
        "eval_now = len(train_loader)//4\n",
        "print(\"\\nEvaluation after every {} Iterations !!!\\n\".format(eval_now))\n",
        "\n",
        "loss_scaler = NativeScaler()\n",
        "torch.cuda.empty_cache()\n",
        "for epoch in range(start_epoch, opt.nepoch + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_loss = 0\n",
        "    train_id = 1\n",
        "\n",
        "    for i, data in enumerate(train_loader, 0): \n",
        "        print(\"Iteration {} of epoch {}\".format(i, epoch))\n",
        "        # zero_grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        target = data[0].cuda()\n",
        "        input_ = data[1].cuda()\n",
        "\n",
        "        if epoch>5:\n",
        "            target, input_ = MixUp_AUG().aug(target, input_)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            restored = model_restoration(input_)\n",
        "            restored = torch.clamp(restored,0,1)  \n",
        "            loss = criterion(restored, target)\n",
        "        loss_scaler(\n",
        "                loss, optimizer,parameters=model_restoration.parameters())\n",
        "        epoch_loss +=loss.item()\n",
        "\n",
        "        #### Evaluation ####\n",
        "        if (i+1)%eval_now==0 and i>0:\n",
        "            with torch.no_grad():\n",
        "                model_restoration.eval()\n",
        "                psnr_val_rgb = []\n",
        "                for ii, data_val in enumerate((val_loader), 0):\n",
        "                    target = data_val[0].cuda()\n",
        "                    input_ = data_val[1].cuda()\n",
        "                    filenames = data_val[2]\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        restored = model_restoration(input_)\n",
        "                    restored = torch.clamp(restored,0,1)  \n",
        "                    psnr_val_rgb.append(batch_PSNR(restored, target, False).item())\n",
        "\n",
        "                psnr_val_rgb = sum(psnr_val_rgb)/len_valset\n",
        "                \n",
        "                if psnr_val_rgb > best_psnr:\n",
        "                    best_psnr = psnr_val_rgb\n",
        "                    best_epoch = epoch\n",
        "                    best_iter = i \n",
        "                    torch.save({'epoch': epoch, \n",
        "                                'state_dict': model_restoration.state_dict(),\n",
        "                                'optimizer' : optimizer.state_dict()\n",
        "                                }, os.path.join(model_dir,\"model_best.pth\"))\n",
        "\n",
        "                print(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr))\n",
        "                with open(logname,'a') as f:\n",
        "                    f.write(\"[Ep %d it %d\\t PSNR SIDD: %.4f\\t] ----  [best_Ep_SIDD %d best_it_SIDD %d Best_PSNR_SIDD %.4f] \" \\\n",
        "                        % (epoch, i, psnr_val_rgb,best_epoch,best_iter,best_psnr)+'\\n')\n",
        "                model_restoration.train()\n",
        "                torch.cuda.empty_cache()\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    print(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0]))\n",
        "    print(\"------------------------------------------------------------------\")\n",
        "    with open(logname,'a') as f:\n",
        "        f.write(\"Epoch: {}\\tTime: {:.4f}\\tLoss: {:.4f}\\tLearningRate {:.6f}\".format(epoch, time.time()-epoch_start_time,epoch_loss, scheduler.get_lr()[0])+'\\n')\n",
        "\n",
        "    torch.save({'epoch': epoch, \n",
        "                'state_dict': model_restoration.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict()\n",
        "                }, os.path.join(model_dir,\"model_latest.pth\"))   \n",
        "\n",
        "    if epoch%opt.checkpoint == 0:\n",
        "        torch.save({'epoch': epoch, \n",
        "                    'state_dict': model_restoration.state_dict(),\n",
        "                    'optimizer' : optimizer.state_dict()\n",
        "                    }, os.path.join(model_dir,\"model_epoch_{}.pth\".format(epoch))) \n",
        "print(\"Now time is : \",datetime.datetime.now().isoformat())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiVKPrYZlI9e"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N DESPUÃ‰S DEL ENTRENAMIENTO (con el mejor modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEUVunDjlI9f"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRSE44ODlI9f"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/results-postrained/'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/data/Confocal_MICE/log/Uformer32/models/model_best.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9qR4ZsilI9g"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIMZ2EaclI9g"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-WPzc64lI9g"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCzXUtCIlI9g"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAKr7H5jlI9g"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4zQmrYllI9g"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYt5fFcZlI9g"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmH-poaZlI9g"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5HeymBxlI9g"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B0ozPDPlI9g"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU5Lq9ChlI9g"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnwRO12qlI9h"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpQEvhI7lI9h"
      },
      "source": [
        "## ðŸ§ª  EVALUACIÃ“N DESPUÃ‰S DEL ENTRENAMIENTO (con el Ãºltimo modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJkDCZ3KlI9h"
      },
      "source": [
        "Constantes de configuraciÃ³n para la evaluaciÃ³n del modelo pre-entrenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwilzao1lI9h"
      },
      "outputs": [],
      "source": [
        "# Arquitectura que vamos a usar para la evaluaciÃ³n\n",
        "arch = 'Uformer'\n",
        "\n",
        "# TamaÃ±o del batch\n",
        "batch_size = 1\n",
        "\n",
        "# GPUs\n",
        "gpu = '0,1'\n",
        "\n",
        "# Ruta en la que se encuentra el input\n",
        "input_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/'\n",
        "\n",
        "# Ruta en la que se almacenarÃ¡ el resultado\n",
        "result_dir = '/content/drive/MyDrive/TFM/data/Confocal_MICE/results-postrained-latest'\n",
        "\n",
        "# Ruta del modelo generado durante el entrenamiento\n",
        "weights = '/content/drive/MyDrive/TFM/data/Confocal_MICE/log/Uformer32/models/model_latest.pth'\n",
        "\n",
        "# TamaÃ±o de los embbedings de las caracterÃ­sticas\n",
        "embed_dim = 32\n",
        "\n",
        "# TamaÃ±o del patch de las muestras de entrenamiento\n",
        "train_ps = 128\n",
        "\n",
        "# TamaÃ±o de ventana para self-attention\n",
        "win_size = 8\n",
        "\n",
        "# Linear/conv token embedding\n",
        "token_embed = 'linear'\n",
        "\n",
        "# FFN/LEFF token mlp\n",
        "token_mlp = 'leff'\n",
        "\n",
        "# Variable para controlar el almacenamiento de las imÃ¡genes sin ruido\n",
        "save_images = 'store_true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfkFJVPelI9h"
      },
      "source": [
        "Generamos las opts que serÃ¡n utilizadas por el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQFi6saFlI9h"
      },
      "outputs": [],
      "source": [
        "class EvalOptions():\n",
        "    def __init__(\n",
        "      self,\n",
        "      arch,\n",
        "      batch_size,\n",
        "      gpu,\n",
        "      input_dir,\n",
        "      result_dir,\n",
        "      weights,\n",
        "      embed_dim,\n",
        "      train_ps,\n",
        "      win_size,\n",
        "      token_embed,\n",
        "      token_mlp,\n",
        "      save_images):\n",
        "        self.arch = arch\n",
        "        self.batch_size = batch_size\n",
        "        self.gpu = gpu\n",
        "        self.input_dir = input_dir\n",
        "        self.result_dir = result_dir\n",
        "        self.weights = weights\n",
        "        self.embed_dim = embed_dim\n",
        "        self.train_ps = train_ps\n",
        "        self.win_size = win_size\n",
        "        self.token_embed = token_embed\n",
        "        self.token_mlp = token_mlp\n",
        "        self.save_images = save_images\n",
        "\n",
        "# Creamos la instancia de la clase\n",
        "opt = EvalOptions(\n",
        "    arch,\n",
        "    batch_size,\n",
        "    gpu,\n",
        "    input_dir,\n",
        "    result_dir,\n",
        "    weights,\n",
        "    embed_dim,\n",
        "    train_ps,\n",
        "    win_size,\n",
        "    token_embed,\n",
        "    token_mlp,\n",
        "    save_images\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E2OaLcZlI9h"
      },
      "source": [
        "Creamos el directorio en el que vamos a guardar los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnmF131_lI9h"
      },
      "outputs": [],
      "source": [
        "mkdir(opt.result_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFSKvuaWlI9h"
      },
      "source": [
        "Cargamos el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay5TFb2xlI9h"
      },
      "outputs": [],
      "source": [
        "test_dataset = get_validation_data(opt.input_dir)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False, num_workers=8, drop_last=False)\n",
        "print(len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "587ESrPZlI9h"
      },
      "source": [
        "Cargamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w6kXdevlI9h"
      },
      "outputs": [],
      "source": [
        "model_restoration= get_arch(opt)\n",
        "model_restoration = torch.nn.DataParallel(model_restoration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8kDizqXlI9i"
      },
      "source": [
        "Cargamos el modelo con los pesos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j9wXh43lI9i"
      },
      "outputs": [],
      "source": [
        "load_checkpoint(model_restoration,opt.weights)\n",
        "print(\"===>Testing using weights: \", opt.weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg-ma_SQlI9i"
      },
      "source": [
        "EvaluaciÃ³n del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHVAOPvxlI9i"
      },
      "outputs": [],
      "source": [
        "model_restoration.cuda()\n",
        "model_restoration.eval()\n",
        "with torch.no_grad():\n",
        "    psnr_val_rgb = []\n",
        "    ssim_val_rgb = []\n",
        "    for ii, data_test in enumerate(tqdm(test_loader), 0):\n",
        "        rgb_gt = data_test[0].numpy().squeeze().transpose((1,2,0))\n",
        "        rgb_noisy = data_test[1].cuda()\n",
        "        filenames = data_test[2]\n",
        "\n",
        "        rgb_restored = model_restoration(rgb_noisy)\n",
        "        rgb_restored = torch.clamp(rgb_restored,0,1).cpu().numpy().squeeze().transpose((1,2,0))\n",
        "        psnr_val_rgb.append(psnr_loss(rgb_restored, rgb_gt))\n",
        "        ssim_val_rgb.append(ssim_loss(rgb_restored, rgb_gt, multichannel=True))\n",
        "\n",
        "        if opt.save_images:\n",
        "            save_img(os.path.join(opt.result_dir,filenames[0]), img_as_ubyte(rgb_restored))\n",
        "\n",
        "psnr_val_rgb = sum(psnr_val_rgb)/len(test_dataset)\n",
        "ssim_val_rgb = sum(ssim_val_rgb)/len(test_dataset)\n",
        "print(\"PSNR: %f, SSIM: %f \" %(psnr_val_rgb,ssim_val_rgb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiWrut4RMi0q"
      },
      "source": [
        "## Imagenes resultantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnThx7OSMW4k"
      },
      "outputs": [],
      "source": [
        "imgInput = plt.imread('/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/input/42_1.png')\n",
        "\n",
        "imgGT = plt.imread('/content/drive/MyDrive/TFM/data/Confocal_MICE/val/cut/groundtruth/42_1.png')\n",
        "\n",
        "imgResult = plt.imread('/content/drive/MyDrive/TFM/data/Confocal_MICE/results-postrained/42_1.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItJJTRd7Mmo8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(label='Input Image')\n",
        "plt.imshow(imgInput[100:200,150:250])\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(label='Groundtruth')\n",
        "plt.imshow(imgGT[100:200,150:250])\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(label='Result')\n",
        "plt.imshow(imgResult[100:200,150:250])\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hTrE-A7XPP0R",
        "me2fEsVbjVAc",
        "nxRP9mlGjy7F"
      ],
      "machine_shape": "hm",
      "name": "tfm.ipynb",
      "toc_visible": true,
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}